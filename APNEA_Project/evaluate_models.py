"""
Script Ä‘Ã¡nh giÃ¡ vÃ  so sÃ¡nh káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n,
kÃ¨m theo cÃ¡c biá»ƒu Ä‘á»“ trá»±c quan Ä‘á»ƒ phÃ¢n tÃ­ch hiá»‡u suáº¥t.
"""
import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from pathlib import Path
import json

# ThÃªm thÆ° má»¥c gá»‘c vÃ o sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import cÃ¡c module
from utils.visualization import (plot_training_history, plot_confusion_matrix, 
                              plot_roc_curve, plot_ahi_comparison, evaluate_ahi_predictions)
from utils.evaluate_AHI import calculate_ahi_from_y_blocks_dir, plot_severity_confusion_matrix
from models.convnext_lstm_lite import ConvNeXtLSTMLite
from models.convnext_transformer_lite import ConvNeXtTransformerLite
from dataset.lazy_apnea_dataset import LazyApneaDataset
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score

def load_model(model_type, checkpoint_path, device):
    """
    Táº£i mÃ´ hÃ¬nh tá»« checkpoint
    """
    if model_type.lower() == 'lstm':
        model = ConvNeXtLSTMLite(num_classes=2).to(device)
    elif model_type.lower() == 'transformer':
        model = ConvNeXtTransformerLite(num_classes=2).to(device)
    else:
        raise ValueError(f"Loáº¡i mÃ´ hÃ¬nh {model_type} khÃ´ng Ä‘Æ°á»£c há»— trá»£")
    
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()
    
    print(f"âœ… ÄÃ£ táº£i mÃ´ hÃ¬nh {model_type} tá»« {checkpoint_path}")
    return model

def predict_dataset(model, data_loader, device):
    """
    Dá»± Ä‘oÃ¡n trÃªn toÃ n bá»™ dataset vÃ  tráº£ vá» nhÃ£n dá»± Ä‘oÃ¡n vÃ  xÃ¡c suáº¥t
    """
    model.eval()
    all_preds = []
    all_probs = []
    all_labels = []
    
    with torch.no_grad():
        for x, y in data_loader:
            x = x.to(device)
            outputs = model(x)
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(1)
            
            all_preds.extend(preds.cpu().numpy())
            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class 1
            all_labels.extend(y.cpu().numpy())
    
    return np.array(all_labels), np.array(all_preds), np.array(all_probs)

def evaluate_model(model, data_loader, device, output_dir, model_name):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh vÃ  lÆ°u cÃ¡c biá»ƒu Ä‘á»“ káº¿t quáº£
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"ğŸ” Äang Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh {model_name}...")
    y_true, y_pred, y_probs = predict_dataset(model, data_loader, device)
    
    # TÃ­nh cÃ¡c metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_probs)
    
    # LÆ°u metrics vÃ o file
    metrics = {
        "model": model_name,
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "roc_auc": float(roc_auc)
    }
    
    metrics_path = os.path.join(output_dir, f"{model_name}_metrics.json")
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=4)
    
    print(f"ğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh {model_name}:")
    print(f"    Accuracy: {accuracy:.4f}")
    print(f"    Precision: {precision:.4f}")
    print(f"    Recall: {recall:.4f}")
    print(f"    F1 Score: {f1:.4f}")
    print(f"    ROC AUC: {roc_auc:.4f}")
    
    # Váº½ confusion matrix
    cm_path = os.path.join(output_dir, f"{model_name}_confusion_matrix.png")
    plot_confusion_matrix(y_true, y_pred, save_path=cm_path)
    
    # Váº½ ROC curve
    roc_path = os.path.join(output_dir, f"{model_name}_roc_curve.png")
    plot_roc_curve(y_true, y_probs, save_path=roc_path)
    
    return metrics

def evaluate_all_patients(model, data_dir, output_dir, model_name, device):
    """
    ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn tá»«ng bá»‡nh nhÃ¢n riÃªng láº» vÃ  tÃ­nh AHI
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # TÃ¬m táº¥t cáº£ cÃ¡c thÆ° má»¥c bá»‡nh nhÃ¢n
    patient_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) 
                   if os.path.isdir(os.path.join(data_dir, d))]
    
    true_ahis = []
    pred_ahis = []
    patient_ids = []
    
    for patient_dir in patient_dirs:
        patient_id = os.path.basename(patient_dir)
        try:
            # Táº¡o dataset cho bá»‡nh nhÃ¢n
            dataset = LazyApneaDataset(patient_dir)
            if len(dataset) == 0:
                continue
                
            dataloader = DataLoader(dataset, batch_size=32, shuffle=False)
            
            # Dá»± Ä‘oÃ¡n
            y_true, y_pred, _ = predict_dataset(model, dataloader, device)
            
            # TÃ­nh AHI
            true_ahi = sum(y_true) / (len(y_true) * 30 / 3600)
            pred_ahi = sum(y_pred) / (len(y_pred) * 30 / 3600)
            
            true_ahis.append(true_ahi)
            pred_ahis.append(pred_ahi)
            patient_ids.append(patient_id)
            
            print(f"âœ… Bá»‡nh nhÃ¢n {patient_id}: True AHI = {true_ahi:.2f}, Pred AHI = {pred_ahi:.2f}")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi xá»­ lÃ½ bá»‡nh nhÃ¢n {patient_id}: {e}")
    
    # Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh AHI
    if true_ahis and pred_ahis:
        ahi_path = os.path.join(output_dir, f"{model_name}_ahi_comparison.png")
        plot_ahi_comparison(true_ahis, pred_ahis, save_path=ahi_path)
        
        # Váº½ confusion matrix má»©c Ä‘á»™ OSA
        severity_cm_path = os.path.join(output_dir, f"{model_name}_osa_severity_cm.png")
        plot_severity_confusion_matrix(true_ahis, pred_ahis, save_path=severity_cm_path)
    
    return true_ahis, pred_ahis, patient_ids

def compare_models(models_metrics, output_dir):
    """
    So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh báº±ng biá»ƒu Ä‘á»“
    """
    models = list(models_metrics.keys())
    
    # So sÃ¡nh cÃ¡c metrics
    metrics = ["accuracy", "precision", "recall", "f1_score", "roc_auc"]
    values = {metric: [models_metrics[model][metric] for model in models] for metric in metrics}
    
    plt.figure(figsize=(15, 8))
    x = np.arange(len(models))
    width = 0.15
    
    for i, metric in enumerate(metrics):
        plt.bar(x + i*width, values[metric], width, label=metric.capitalize())
    
    plt.xlabel('MÃ´ hÃ¬nh')
    plt.ylabel('Score')
    plt.title('So sÃ¡nh hiá»‡u suáº¥t cÃ¡c mÃ´ hÃ¬nh')
    plt.xticks(x + width*2, models)
    plt.legend()
    plt.ylim(0, 1)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "model_comparison.png"), dpi=300, bbox_inches='tight')
    print(f"âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“ so sÃ¡nh táº¡i {os.path.join(output_dir, 'model_comparison.png')}")

def main(args):
    # Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n
    checkpoint_dir = args.checkpoint_dir
    data_dir = args.data_dir
    output_dir = args.output_dir
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Thiáº¿t láº­p device
    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    print(f"ğŸ–¥ï¸ Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")
    
    # CÃ¡c mÃ´ hÃ¬nh cáº§n Ä‘Ã¡nh giÃ¡
    models_to_evaluate = []
    
    if args.lstm:
        lstm_path = os.path.join(checkpoint_dir, "ConvNeXtLSTMLite_best.pth")
        if os.path.exists(lstm_path):
            models_to_evaluate.append(("lstm", lstm_path))
    
    if args.transformer:
        transformer_path = os.path.join(checkpoint_dir, "ConvNeXtTransformerLite_best.pth")
        if os.path.exists(transformer_path):
            models_to_evaluate.append(("transformer", transformer_path))
    
    # ÄÃ¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh
    all_metrics = {}
    
    for model_type, checkpoint_path in models_to_evaluate:
        model = load_model(model_type, checkpoint_path, device)
        model_output_dir = os.path.join(output_dir, model_type)
        os.makedirs(model_output_dir, exist_ok=True)
        
        # ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn toÃ n bá»™ táº­p dá»¯ liá»‡u
        print(f"\n===== ÄÃNH GIÃ MÃ” HÃŒNH {model_type.upper()} =====")
        
        # ÄÃ¡nh giÃ¡ AHI trÃªn tá»«ng bá»‡nh nhÃ¢n
        print(f"\nğŸ©º ÄÃ¡nh giÃ¡ AHI trÃªn tá»«ng bá»‡nh nhÃ¢n...")
        true_ahis, pred_ahis, patient_ids = evaluate_all_patients(
            model, data_dir, model_output_dir, model_type, device
        )
        
        if true_ahis and pred_ahis:
            # TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ AHI
            mae = np.mean(np.abs(np.array(pred_ahis) - np.array(true_ahis)))
            rmse = np.sqrt(np.mean(np.square(np.array(pred_ahis) - np.array(true_ahis))))
            corr = np.corrcoef(true_ahis, pred_ahis)[0, 1]
            
            print(f"ğŸ“Š ÄÃ¡nh giÃ¡ AHI cho {model_type}:")
            print(f"    MAE: {mae:.4f}")
            print(f"    RMSE: {rmse:.4f}")
            print(f"    Correlation: {corr:.4f}")
            
            # ThÃªm vÃ o metrics
            metrics = {
                "model": model_type,
                "mae": float(mae),
                "rmse": float(rmse),
                "correlation": float(corr)
            }
            
            all_metrics[model_type] = metrics
    
    # So sÃ¡nh cÃ¡c mÃ´ hÃ¬nh
    if len(all_metrics) > 1:
        compare_models(all_metrics, output_dir)
    
    print(f"\nâœ… ÄÃ¡nh giÃ¡ hoÃ n táº¥t. Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i {output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh phÃ¡t hiá»‡n Apnea")
    parser.add_argument('--data_dir', type=str, default='data/blocks',
                        help='ThÆ° má»¥c chá»©a dá»¯ liá»‡u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡')
    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints',
                        help='ThÆ° má»¥c chá»©a cÃ¡c model checkpoint')
    parser.add_argument('--output_dir', type=str, default='evaluation_results',
                        help='ThÆ° má»¥c Ä‘á»ƒ lÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡')
    parser.add_argument('--lstm', action='store_true', help='ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh LSTM')
    parser.add_argument('--transformer', action='store_true', help='ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh Transformer')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'],
                        help='Thiáº¿t bá»‹ Ä‘á»ƒ cháº¡y Ä‘Ã¡nh giÃ¡ (cuda hoáº·c cpu)')
    
    args = parser.parse_args()
    
    if not (args.lstm or args.transformer):
        # Máº·c Ä‘á»‹nh Ä‘Ã¡nh giÃ¡ cáº£ hai mÃ´ hÃ¬nh náº¿u khÃ´ng chá»‰ Ä‘á»‹nh
        args.lstm = True
        args.transformer = True
    
    main(args)
