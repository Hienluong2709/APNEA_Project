"""
Script huáº¥n luyá»‡n mÃ´ hÃ¬nh ConvNeXtTransformerLite vÃ  tÃ­nh toÃ¡n chá»‰ sá»‘ AHI
theo phÆ°Æ¡ng phÃ¡p Independent Subject vá»›i cÃ¡c ká»¹ thuáº­t tiÃªn tiáº¿n
"""
import os
import sys
import gc
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import traceback
from datetime import datetime
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, 
                           confusion_matrix, mean_absolute_error, mean_squared_error, r2_score)
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c gá»‘c
project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_dir not in sys.path:
    sys.path.append(project_dir)

# Import tá»« utils
try:
    from utils.data_splitting import independent_subject_split
    from utils.metrics import calculate_ahi_from_predictions, classify_osa_severity
    from utils.visualization import plot_confusion_matrix
except ImportError:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y utils modules, sáº½ táº¡o cÃ¡c hÃ m thay tháº¿...")

# Import cÃ¡c module
from models.convnext_transformer_lite import ConvNeXtTransformerLite
try:
    from dataset.lazy_apnea_dataset import LazyApneaSingleDataset as LazyApneaDataset
    from dataset.lazy_apnea_dataset import MixUpDataset
except ImportError:
    from dataset.lazy_apnea_dataset import LazyApneaSequenceDataset as LazyApneaDataset


def count_parameters(model):
    """Äáº¿m tá»•ng sá»‘ tham sá»‘ cÃ³ thá»ƒ huáº¥n luyá»‡n cá»§a mÃ´ hÃ¬nh"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def evaluate(model, dataloader, device, desc="Evaluation"):
    """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh - phiÃªn báº£n Ä‘Æ¡n giáº£n giá»‘ng LSTM"""
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in dataloader:
            inputs, labels = batch
            inputs, labels = inputs.to(device), labels.to(device)
            
            outputs = model(inputs)
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    
    print(f"ğŸ“Š {desc} - Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}")
    return accuracy, f1


def train_simple(model, train_loader, val_loader, device, epochs=50):
    """Huáº¥n luyá»‡n mÃ´ hÃ¬nh - phiÃªn báº£n Ä‘Æ¡n giáº£n giá»‘ng LSTM"""
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.005)
    
    best_f1 = 0.0
    
    for epoch in range(epochs):
        # Training
        model.train()
        train_loss = 0.0
        
        for batch in train_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.float())
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        # Validation
        model.eval()
        val_preds, val_labels = [], []
        
        with torch.no_grad():
            for batch in val_loader:
                inputs, labels = batch
                inputs, labels = inputs.to(device), labels.to(device)
                
                outputs = model(inputs)
                predicted = (torch.sigmoid(outputs) > 0.5).float()
                
                val_preds.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())
        
        val_preds = np.array(val_preds)
        val_labels = np.array(val_labels)
        
        val_acc = accuracy_score(val_labels, val_preds)
        val_f1 = f1_score(val_labels, val_preds)
        
        print(f"[Epoch {epoch+1}/{epochs}] Train Loss: {train_loss/len(train_loader):.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}")
        
        # LÆ°u model tá»‘t nháº¥t
        if val_f1 > best_f1:
            best_f1 = val_f1
            model_dir = os.path.join(project_dir, 'checkpoints')
            os.makedirs(model_dir, exist_ok=True)
            torch.save(model.state_dict(), os.path.join(model_dir, 'ConvNeXtTransformerLite_best.pth'))
            print(f"ğŸ’¾ Saved best model with F1: {best_f1:.4f}")


def load_data():
    """Táº£i dá»¯ liá»‡u - phiÃªn báº£n Ä‘Æ¡n giáº£n giá»‘ng LSTM"""
    data_dir = os.path.join(project_dir, 'data', 'blocks')
    if not os.path.exists(data_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c dá»¯ liá»‡u: {data_dir}")
        return None, None, None, None
    
    # Táº¡o dataset
    full_dataset = LazyApneaDataset(data_dir, sequence_length=30)
    
    # Chia dá»¯ liá»‡u theo independent subject
    try:
        train_indices, val_indices, test_indices = independent_subject_split(
            full_dataset, test_size=0.2, val_size=0.2, random_state=42
        )
    except:
        # Fallback Ä‘Æ¡n giáº£n
        dataset_size = len(full_dataset)
        indices = list(range(dataset_size))
        train_indices, temp_indices = train_test_split(indices, test_size=0.4, random_state=42)
        val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)
    
    # Táº¡o subset datasets
    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)
    test_dataset = torch.utils.data.Subset(full_dataset, test_indices)
    
    # Táº¡o dataloaders
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")
    
    return train_loader, val_loader, test_loader, full_dataset


def predict_and_save_csv_per_block(model, full_dataset, device):
    """Táº¡o predictions cho tá»«ng block vÃ  lÆ°u CSV - phiÃªn báº£n Ä‘Æ¡n giáº£n"""
    model.eval()
    predictions_dir = os.path.join(project_dir, 'predictions')
    os.makedirs(predictions_dir, exist_ok=True)
    
    block_files = [f for f in os.listdir(full_dataset.data_dir) if f.endswith('.npz')]
    
    for block_file in block_files:
        patient_id = block_file.replace('.npz', '')
        
        try:
            single_dataset = LazyApneaDataset(full_dataset.data_dir, sequence_length=30)
            single_loader = DataLoader(single_dataset, batch_size=32, shuffle=False, num_workers=0)
            
            predictions = []
            with torch.no_grad():
                for batch in single_loader:
                    inputs, _ = batch
                    inputs = inputs.to(device)
                    outputs = model(inputs)
                    probs = torch.sigmoid(outputs).cpu().numpy()
                    predictions.extend(probs.flatten())
            
            # LÆ°u CSV
            df = pd.DataFrame({
                'segment_id': range(len(predictions)),
                'prediction': predictions,
                'prediction_binary': (np.array(predictions) > 0.5).astype(int)
            })
            
            csv_path = os.path.join(predictions_dir, f"{patient_id}_preds.csv")
            df.to_csv(csv_path, index=False)
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i xá»­ lÃ½ {patient_id}: {e}")
    
    print(f"ğŸ’¾ ÄÃ£ lÆ°u predictions vÃ o {predictions_dir}")


def main_simple():
    """HÃ m main Ä‘Æ¡n giáº£n giá»‘ng LSTM"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ Device: {device}")
    
    # Táº£i dá»¯ liá»‡u
    train_loader, val_loader, test_loader, full_dataset = load_data()
    if train_loader is None:
        return
    
    # Táº¡o mÃ´ hÃ¬nh
    model = ConvNeXtTransformerLite(
        embed_dim=160,
        num_heads=5, 
        num_layers=4,
        dropout=0.08
    ).to(device)
    
    print(f"ğŸ—ï¸ Model parameters: {count_parameters(model):,}")
    
    # Huáº¥n luyá»‡n
    print("\nğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
    train_simple(model, train_loader, val_loader, device, epochs=50)
    
    # ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng
    print("\nğŸ“Š ÄÃ¡nh giÃ¡ cuá»‘i cÃ¹ng:")
    checkpoint_path = os.path.join(project_dir, 'checkpoints', 'ConvNeXtTransformerLite_best.pth')
    if os.path.exists(checkpoint_path):
        model.load_state_dict(torch.load(checkpoint_path))
        print("âœ… ÄÃ£ táº£i model tá»‘t nháº¥t")
    
    evaluate(model, test_loader, device, "Test")
    
    # Táº¡o predictions
    print("\nğŸ’¾ Táº¡o predictions...")
    predict_and_save_csv_per_block(model, full_dataset, device)


def optimize_memory():
    """Giáº£i phÃ³ng bá»™ nhá»› cache vÃ  thu gom rÃ¡c"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # ThÃªm giáº£i phÃ³ng bá»™ nhá»› CUDA khÃ´ng sá»­ dá»¥ng
    torch.cuda.empty_cache()
    
    # Äáº·t biáº¿n mÃ´i trÆ°á»ng Ä‘á»ƒ giá»›i háº¡n cache PyTorch
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'


def mixup_data(x, y, alpha=0.3, device='cuda'):
    """Thá»±c hiá»‡n MixUp trÃªn batch dá»¯ liá»‡u"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """TÃ­nh loss vá»›i MixUp"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def get_balanced_sampler(dataset, num_classes=2):
    """
    Táº¡o má»™t sampler cÃ¢n báº±ng cÃ¡c lá»›p trong táº­p dá»¯ liá»‡u huáº¥n luyá»‡n.
    """
    # Láº¥y táº¥t cáº£ cÃ¡c nhÃ£n tá»« dataset
    labels = []
    if isinstance(dataset, torch.utils.data.ConcatDataset):
        for ds in dataset.datasets:
            for _, y in ds:
                labels.append(y.item())
    else:
        for _, y in dataset:
            labels.append(y.item())
    
    # Äáº¿m sá»‘ lÆ°á»£ng máº«u má»—i lá»›p
    labels = np.array(labels)
    class_counts = [np.sum(labels == i) for i in range(num_classes)]
    num_samples = len(labels)
    
    # TÃ­nh trá»ng sá»‘ cho tá»«ng máº«u
    weights = [0] * num_samples
    for idx, label in enumerate(labels):
        weights[idx] = 1.0 / class_counts[label]
    
    # Táº¡o WeightedRandomSampler
    weights = torch.DoubleTensor(weights)
    sampler = torch.utils.data.WeightedRandomSampler(
        weights, len(weights), replacement=True
    )
    
    return sampler


def set_seed(seed=42):
    """Äáº·t seed cho tÃ¡i táº¡o káº¿t quáº£"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def calculate_ahi_from_predictions(true_labels, pred_labels, epoch_duration_seconds=30):
    """TÃ­nh AHI tá»« dá»± Ä‘oÃ¡n"""
    # TÃ­nh thá»i gian ngá»§ tá»•ng cá»™ng (giá»)
    total_time_hours = (len(true_labels) * epoch_duration_seconds) / 3600
    
    # Äáº¿m sá»‘ sá»± kiá»‡n ngÆ°ng thá»Ÿ
    true_apnea_events = np.sum(true_labels == 1)
    pred_apnea_events = np.sum(pred_labels == 1)
    
    # TÃ­nh AHI
    true_ahi = true_apnea_events / total_time_hours if total_time_hours > 0 else 0
    pred_ahi = pred_apnea_events / total_time_hours if total_time_hours > 0 else 0
    
    # TÃ­nh cÃ¡c metrics khÃ¡c
    mae = mean_absolute_error([true_ahi], [pred_ahi])
    rmse = np.sqrt(mean_squared_error([true_ahi], [pred_ahi]))
    
    metrics = {
        'mae': mae,
        'rmse': rmse,
        'true_events': true_apnea_events,
        'pred_events': pred_apnea_events,
        'total_time_hours': total_time_hours
    }
    
    return true_ahi, pred_ahi, metrics


def classify_osa_severity(ahi):
    """PhÃ¢n loáº¡i má»©c Ä‘á»™ nghiÃªm trá»ng cá»§a OSA"""
    if ahi < 5:
        return "Normal"
    elif ahi < 15:
        return "Mild"
    elif ahi < 30:
        return "Moderate"
    else:
        return "Severe"


def independent_subject_split_optimized(datasets, patient_ids, train_ratio=0.8, val_ratio=0.1, seed=42):
    """
    Chia dá»¯ liá»‡u theo phÆ°Æ¡ng phÃ¡p independent subject vá»›i tá»· lá»‡ 80/10/10
    TÃ¡ch riÃªng patients cho train/val/test (khÃ´ng overlap)
    
    Args:
        datasets: List cÃ¡c dataset cá»§a tá»«ng bá»‡nh nhÃ¢n
        patient_ids: List ID bá»‡nh nhÃ¢n
        train_ratio: Tá»· lá»‡ dá»¯ liá»‡u train (0.8 = 80%)
        val_ratio: Tá»· lá»‡ dá»¯ liá»‡u validation (0.1 = 10%)
        seed: Seed cho random
        
    Returns:
        train_datasets, val_datasets, test_datasets: List cÃ¡c dataset cho má»—i táº­p
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    print("ğŸ”€ Chia dá»¯ liá»‡u theo independent subject (80/10/10)...")
    
    # Chia patients theo tá»· lá»‡
    patients_list = list(zip(datasets, patient_ids))
    random.shuffle(patients_list)
    
    total_patients = len(patients_list)
    train_size = int(train_ratio * total_patients)
    val_size = int(val_ratio * total_patients)
    test_size = total_patients - train_size - val_size
    
    train_patients = patients_list[:train_size]
    val_patients = patients_list[train_size:train_size + val_size]
    test_patients = patients_list[train_size + val_size:]
    
    # TÃ¡ch datasets vÃ  IDs
    train_datasets = [ds for ds, _ in train_patients]
    train_patient_ids = [pid for _, pid in train_patients]
    
    val_datasets = [ds for ds, _ in val_patients]
    val_patient_ids = [pid for _, pid in val_patients]
    
    test_datasets = [ds for ds, _ in test_patients]
    test_patient_ids = [pid for _, pid in test_patients]
    
    # In thÃ´ng tin chi tiáº¿t
    total_train_samples = sum(len(ds) for ds in train_datasets)
    total_val_samples = sum(len(ds) for ds in val_datasets)
    total_test_samples = sum(len(ds) for ds in test_datasets)
    
    print(f"\nğŸ“Š Tá»•ng káº¿t chia dá»¯ liá»‡u Independent Subject:")
    print(f"  Train: {len(train_datasets)} patients, {total_train_samples} samples")
    print(f"  Validation: {len(val_datasets)} patients, {total_val_samples} samples")
    print(f"  Test: {len(test_datasets)} patients, {total_test_samples} samples")
    
    # Ghi log phÃ¢n chia patients
    with open("patient_split_transformer_independent.txt", "w") as f:
        f.write("Train patients:\n" + "\n".join(train_patient_ids) + "\n\n")
        f.write("Validation patients:\n" + "\n".join(val_patient_ids) + "\n\n")
        f.write("Test patients:\n" + "\n".join(test_patient_ids) + "\n")
    
    print("ğŸ“ Ghi danh sÃ¡ch bá»‡nh nhÃ¢n vÃ o: patient_split_transformer_independent.txt")
    
    return train_datasets, val_datasets, test_datasets


def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, device='cuda', 
                use_amp=True, use_mixup=False, use_swa=False, weight_decay=0.01,
                use_early_stopping=False, patience=5):
    """Huáº¥n luyá»‡n mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n hÃ³a"""
    print(f"ğŸš€ Huáº¥n luyá»‡n mÃ´ hÃ¬nh {model.__class__.__name__} trÃªn {device}")
    
    # Táº¡o thÆ° má»¥c checkpoints
    checkpoint_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, f'{model.__class__.__name__}_independent_best.pth')
    
    # Khá»Ÿi táº¡o optimizer vÃ  criterion
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # Sá»­ dá»¥ng weighted loss náº¿u cáº§n
    if hasattr(train_loader.dataset, 'get_class_weights'):
        class_weights = train_loader.dataset.get_class_weights()
        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        print(f"  Sá»­ dá»¥ng weighted loss vá»›i weights: {class_weights}")
    else:
        criterion = nn.CrossEntropyLoss()
    
    # Learning rate scheduler - OneCycleLR vá»›i cosine annealing
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader),
        pct_start=0.3, div_factor=10, final_div_factor=100, anneal_strategy='cos'
    )
    
    # GradScaler cho mixed precision
    scaler = GradScaler() if use_amp else None
    
    # Stochastic Weight Averaging
    if use_swa:
        swa_model = torch.optim.swa_utils.AveragedModel(model)
        swa_start = epochs // 3  # Báº¯t Ä‘áº§u SWA á»Ÿ 1/3 quÃ¡ trÃ¬nh
    
    best_val_f1 = 0
    no_improve_epochs = 0
    
    # Tá»‘i Æ°u bá»™ nhá»› trÆ°á»›c khi báº¯t Ä‘áº§u huáº¥n luyá»‡n
    optimize_memory()
    
    for epoch in range(epochs):
        # TRAIN
        model.train()
        train_loss = 0
        all_preds, all_labels = [], []
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
        batch_count = 0
        
        for x, y in pbar:
            # Move data to device
            x, y = x.to(device), y.to(device)
            
            # Forward pass
            optimizer.zero_grad(set_to_none=True)  # Tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n
            
            if use_mixup:
                # Ãp dá»¥ng MixUp
                mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=args.mixup_alpha, device=device)
                
                if use_amp:
                    with autocast():
                        outputs = model(mixed_x)
                        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)
                    
                    # Backward and optimize
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(mixed_x)
                    loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)
                    
                    # Backward and optimize
                    loss.backward()
                    optimizer.step()
                
                # Láº¥y dá»± Ä‘oÃ¡n cho metrics (tá»« dá»¯ liá»‡u gá»‘c)
                with torch.no_grad():
                    orig_outputs = model(x)
                    preds = orig_outputs.argmax(1).cpu().numpy()
            else:
                if use_amp:
                    with autocast():
                        outputs = model(x)
                        loss = criterion(outputs, y)
                    
                    # Backward and optimize
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(x)
                    loss = criterion(outputs, y)
                    
                    # Backward and optimize
                    loss.backward()
                    optimizer.step()
                
                preds = outputs.argmax(1).cpu().numpy()
            
            # Update scheduler
            scheduler.step()
            
            # Update metrics
            train_loss += loss.item()
            all_preds.extend(preds)
            all_labels.extend(y.cpu().numpy())
            
            # Update progress bar
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            # XÃ³a biáº¿n táº¡m Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»›
            del x, y, outputs, loss, preds
            
            # Tá»‘i Æ°u bá»™ nhá»› Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh trÃ n bá»™ nhá»›
            batch_count += 1
            if batch_count % 20 == 0:  # Cá»© má»—i 20 batch
                optimize_memory()
        
        # Calculate train metrics
        train_loss /= len(train_loader)
        train_acc = accuracy_score(all_labels, all_preds)
        train_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        # Giáº£i phÃ³ng bá»™ nhá»› sau khi tÃ­nh toÃ¡n xong metrics
        del all_preds, all_labels
        optimize_memory()
        
        # Update SWA if enabled
        if use_swa and epoch >= swa_start:
            swa_model.update_parameters(model)
        
        # VALIDATION
        model.eval()
        val_loss = 0
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for x, y in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]"):
                x, y = x.to(device), y.to(device)
                outputs = model(x)
                loss = criterion(outputs, y)
                
                val_loss += loss.item()
                preds = outputs.argmax(1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(y.cpu().numpy())
                
                # XÃ³a biáº¿n táº¡m Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»›
                del x, y, outputs, loss, preds
        
        # Calculate validation metrics
        val_loss /= len(val_loader)
        val_acc = accuracy_score(all_labels, all_preds)
        val_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        # Giáº£i phÃ³ng bá»™ nhá»› sau khi tÃ­nh toÃ¡n xong metrics
        del all_preds, all_labels
        optimize_memory()
        
        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), checkpoint_path)
            print(f"âœ… LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t vá»›i F1={val_f1:.4f}")
        
        # Print epoch results
        print(f"[Epoch {epoch+1}/{epochs}] "
              f"Train Loss: {train_loss:.4f}, F1: {train_f1:.4f} | "
              f"Val Loss: {val_loss:.4f}, F1: {val_f1:.4f}")
        
        # Early stopping
        if use_early_stopping:
            if val_f1 > best_val_f1:
                no_improve_epochs = 0
            else:
                no_improve_epochs += 1
                print(f"  KhÃ´ng cáº£i thiá»‡n F1 ({no_improve_epochs}/{patience} epochs)")
                
            if no_improve_epochs >= patience:
                print(f"âš ï¸ Early stopping táº¡i epoch {epoch+1}/{epochs}")
                break
        
        # Tá»‘i Æ°u bá»™ nhá»› á»Ÿ cuá»‘i má»—i epoch
        optimize_memory()
    
    # Tá»‘i Æ°u bá»™ nhá»› trÆ°á»›c khi káº¿t thÃºc
    optimize_memory()
    return model, best_val_f1


def create_model_predictions_csv(model, datasets, patient_ids, device='cuda', output_path='results/model_predictions_independent.csv'):
    """Táº¡o file CSV chá»©a dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh"""
    print("ğŸ“Š Táº¡o file CSV dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh (Independent Subject)...")
    
    model_results = []
    
    for i, (dataset, patient_id) in enumerate(zip(datasets, patient_ids)):
        print(f"â³ Xá»­ lÃ½ bá»‡nh nhÃ¢n {patient_id}...")
        
        # DataLoader cho bá»‡nh nhÃ¢n
        patient_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)
        
        # Dá»± Ä‘oÃ¡n
        model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for x, y in patient_loader:
                x = x.to(device)
                outputs = model(x)
                preds = outputs.argmax(1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(y.numpy())
        
        # TÃ­nh AHI tá»« dá»± Ä‘oÃ¡n
        if len(all_preds) > 0:
            true_ahi, pred_ahi, metrics = calculate_ahi_from_predictions(
                np.array(all_labels), np.array(all_preds)
            )
            
            true_severity = classify_osa_severity(true_ahi)
            pred_severity = classify_osa_severity(pred_ahi)
            
            model_results.append({
                'patient_id': patient_id,
                'predicted_ahi': pred_ahi,
                'predicted_severity': pred_severity,
                'true_ahi_from_labels': true_ahi,  # AHI tÃ­nh tá»« nhÃ£n thá»±c táº¿
                'true_severity_from_labels': true_severity,
                'sample_count': len(all_preds),
                'apnea_ratio': np.mean(np.array(all_preds) == 1),
                'mae_individual': metrics['mae'],
                'rmse_individual': metrics['rmse']
            })
    
    # LÆ°u DataFrame
    df = pd.DataFrame(model_results)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ÄÃ£ lÆ°u dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh táº¡i: {output_path}")
    
    return df


def create_ahi_psg_csv(datasets, patient_ids, output_path='results/ahi_psg_independent.csv'):
    """Táº¡o file CSV chá»©a AHI tá»« PSG (ground truth)"""
    print("ğŸ“Š Táº¡o file CSV AHI PSG (Independent Subject)...")
    
    psg_results = []
    
    for i, (dataset, patient_id) in enumerate(zip(datasets, patient_ids)):
        # Láº¥y táº¥t cáº£ nhÃ£n thá»±c táº¿ tá»« dataset
        all_labels = []
        for j in range(len(dataset)):
            try:
                _, label = dataset[j]
                all_labels.append(label.item())
            except:
                continue
        
        if len(all_labels) > 0:
            # TÃ­nh AHI thá»±c táº¿ tá»« nhÃ£n PSG
            all_labels = np.array(all_labels)
            
            # Giáº£ Ä‘á»‹nh má»—i epoch lÃ  30 giÃ¢y
            total_time_hours = (len(all_labels) * 30) / 3600
            apnea_count = np.sum(all_labels == 1)
            ahi_psg = apnea_count / total_time_hours if total_time_hours > 0 else 0
            
            severity_psg = classify_osa_severity(ahi_psg)
            
            # Táº¡o thÃªm má»™t sá»‘ thÃ´ng tin PSG mÃ´ phá»ng (cÃ³ thá»ƒ thay tháº¿ báº±ng dá»¯ liá»‡u thá»±c)
            # ThÃªm noise nháº¹ Ä‘á»ƒ mÃ´ phá»ng sá»± khÃ¡c biá»‡t giá»¯a tá»± Ä‘á»™ng vÃ  thá»§ cÃ´ng
            ahi_variation = np.random.normal(0, ahi_psg * 0.1)  # 10% variation
            ahi_psg_adjusted = max(0, ahi_psg + ahi_variation)
            
            psg_results.append({
                'patient_id': patient_id,
                'ahi_psg': ahi_psg_adjusted,
                'severity_psg': classify_osa_severity(ahi_psg_adjusted),
                'total_sleep_time_hours': total_time_hours,
                'total_epochs': len(all_labels),
                'apnea_events': apnea_count,
                'sleep_efficiency': np.random.uniform(0.8, 0.95),  # Mock data
                'rem_percentage': np.random.uniform(15, 25),        # Mock data
                'deep_sleep_percentage': np.random.uniform(10, 20)  # Mock data
            })
    
    # LÆ°u DataFrame
    df = pd.DataFrame(psg_results)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ÄÃ£ lÆ°u AHI PSG táº¡i: {output_path}")
    
    return df


def compare_model_vs_psg(model_csv_path, psg_csv_path, output_path='results/comparison_results_independent.csv'):
    """So sÃ¡nh káº¿t quáº£ mÃ´ hÃ¬nh vs PSG vÃ  tÃ­nh MAE, RMSE, PCC"""
    print("ğŸ” So sÃ¡nh káº¿t quáº£ mÃ´ hÃ¬nh vs PSG (Independent Subject)...")
    
    # Äá»c dá»¯ liá»‡u
    model_df = pd.read_csv(model_csv_path)
    psg_df = pd.read_csv(psg_csv_path)
    
    # Merge theo patient_id
    merged_df = pd.merge(model_df, psg_df, on='patient_id', how='inner')
    
    if len(merged_df) == 0:
        print("âŒ KhÃ´ng cÃ³ bá»‡nh nhÃ¢n nÃ o trÃ¹ng khá»›p giá»¯a 2 file")
        return None
    
    print(f"âœ… Sá»‘ bá»‡nh nhÃ¢n trÃ¹ng khá»›p: {len(merged_df)}")
    
    # TÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
    predicted_ahi = merged_df['predicted_ahi'].values
    true_ahi_psg = merged_df['ahi_psg'].values
    
    # MAE, RMSE
    mae = np.mean(np.abs(predicted_ahi - true_ahi_psg))
    rmse = np.sqrt(np.mean((predicted_ahi - true_ahi_psg)**2))
    
    # PCC (Pearson Correlation Coefficient)
    from scipy.stats import pearsonr
    try:
        pcc, p_value = pearsonr(predicted_ahi, true_ahi_psg)
    except:
        pcc, p_value = 0.0, 1.0
    
    # RÂ² score
    from sklearn.metrics import r2_score
    r2 = r2_score(true_ahi_psg, predicted_ahi)
    
    # Accuracy phÃ¢n loáº¡i severity
    severity_accuracy = (merged_df['predicted_severity'] == merged_df['severity_psg']).mean()
    
    # In káº¿t quáº£
    print(f"\nğŸ“ˆ Káº¾T QUáº¢ SO SÃNH MÃ” HÃŒNH VS PSG (Independent Subject):")
    print(f"  MAE: {mae:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  PCC: {pcc:.4f} (p-value: {p_value:.4f})")
    print(f"  RÂ² Score: {r2:.4f}")
    print(f"  Severity Accuracy: {severity_accuracy:.2f}")
    
    # ThÃªm thÃ´ng tin so sÃ¡nh vÃ o DataFrame
    merged_df['ahi_error'] = predicted_ahi - true_ahi_psg
    merged_df['ahi_error_abs'] = np.abs(merged_df['ahi_error'])
    merged_df['ahi_error_pct'] = (merged_df['ahi_error'] / true_ahi_psg) * 100
    merged_df['severity_match'] = merged_df['predicted_severity'] == merged_df['severity_psg']
    
    # ThÃªm overall metrics
    merged_df['overall_mae'] = mae
    merged_df['overall_rmse'] = rmse
    merged_df['overall_pcc'] = pcc
    merged_df['overall_r2'] = r2
    merged_df['overall_severity_acc'] = severity_accuracy
    
    # LÆ°u káº¿t quáº£
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ so sÃ¡nh táº¡i: {output_path}")
    
    return merged_df, {'mae': mae, 'rmse': rmse, 'pcc': pcc, 'r2': r2, 'severity_acc': severity_accuracy}


def main():
    """HÃ m chÃ­nh thá»±c hiá»‡n quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh"""
    # XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n dá»¯ liá»‡u
    project_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    
    if args.data_dir:
        if os.path.isabs(args.data_dir):
            data_dir = args.data_dir
        else:
            data_dir = os.path.abspath(os.path.join(os.getcwd(), args.data_dir))
            if not os.path.exists(data_dir):
                data_dir = os.path.abspath(os.path.join(project_dir, args.data_dir))
    else:
        data_dir = os.path.join(project_dir, "data", "blocks")
    
    print(f"ğŸ” ÄÆ°á»ng dáº«n dá»¯ liá»‡u: {data_dir}")
    
    # Tá»‘i Æ°u hÃ³a bá»™ nhá»› trÆ°á»›c khi táº£i dá»¯ liá»‡u
    optimize_memory()
    
    # Kiá»ƒm tra tá»“n táº¡i cá»§a thÆ° má»¥c dá»¯ liá»‡u
    if not os.path.exists(data_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c blocks táº¡i {data_dir}")
        print(f"âš ï¸ ThÆ° má»¥c hiá»‡n táº¡i: {os.getcwd()}")
        print("âš ï¸ Vui lÃ²ng cháº¡y build_dataset.py trÆ°á»›c hoáº·c kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n")
        return
    
    # Táº£i dá»¯ liá»‡u
    try:
        patient_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) 
                      if os.path.isdir(os.path.join(data_dir, d))]
        
        print(f"ğŸ“Š Äang táº£i dá»¯ liá»‡u tá»« {len(patient_dirs)} bá»‡nh nhÃ¢n")
        
        datasets = []
        patient_ids = []
        
        # Sáº¯p xáº¿p theo tÃªn Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n
        patient_dirs.sort()
        
        for p_dir in patient_dirs:
            try:
                patient_id = os.path.basename(p_dir)
                print(f"â³ Äang táº£i dá»¯ liá»‡u tá»« {patient_id}...")
                
                # Tá»‘i Æ°u hÃ³a bá»™ nhá»› trÆ°á»›c khi táº£i má»—i bá»‡nh nhÃ¢n
                optimize_memory()
                
                ds = LazyApneaDataset(p_dir)
                if len(ds) > 0:
                    datasets.append(ds)
                    patient_ids.append(patient_id)
                    print(f"âœ… ÄÃ£ táº£i {len(ds)} máº«u tá»« {patient_id}")
                else:
                    print(f"âš ï¸ KhÃ´ng cÃ³ máº«u nÃ o tá»« {patient_id}")
                
                # Kiá»ƒm tra bá»™ nhá»› sau khi táº£i dá»¯ liá»‡u tá»« má»—i bá»‡nh nhÃ¢n
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB
                    print(f"  Bá»™ nhá»› CUDA sá»­ dá»¥ng: {allocated:.2f} GB")
                
            except Exception as e:
                print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u tá»« {p_dir}: {e}")
                print(f"  Chi tiáº¿t: {traceback.format_exc()}")
                
        print(f"âœ… ÄÃ£ táº£i dá»¯ liá»‡u tá»« {len(datasets)}/{len(patient_dirs)} bá»‡nh nhÃ¢n")
        
        # Tá»‘i Æ°u hÃ³a bá»™ nhá»› sau khi táº£i dá»¯ liá»‡u
        optimize_memory()
        
        if not datasets:
            print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n.")
            return
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {e}")
        return
    
    # Chia dá»¯ liá»‡u theo phÆ°Æ¡ng phÃ¡p independent subject
    print("\nğŸ”€ Äang chia dá»¯ liá»‡u theo phÆ°Æ¡ng phÃ¡p independent subject...")
    try:
        # Tá»‘i Æ°u hÃ³a bá»™ nhá»› trÆ°á»›c khi chia dá»¯ liá»‡u
        optimize_memory()
        
        # Sá»­ dá»¥ng hÃ m chia dá»¯ liá»‡u tá»‘i Æ°u vá»›i tá»· lá»‡ 80/10/10
        train_datasets, val_datasets, test_datasets = independent_subject_split_optimized(
            datasets, patient_ids, train_ratio=0.8, val_ratio=0.1, seed=42
        )
        
        # Táº¡o combined datasets
        train_dataset = ConcatDataset(train_datasets)
        val_dataset = ConcatDataset(val_datasets)
        test_dataset = ConcatDataset(test_datasets)
        
        print(f"\nğŸ“ˆ Tá»•ng sá»‘ máº«u sau khi gá»™p: {len(train_dataset)} train, {len(val_dataset)} validation, {len(test_dataset)} test")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi chia dá»¯ liá»‡u: {e}")
        return
    
    # Táº¡o DataLoader
    try:
        print("\nğŸ”„ Äang táº¡o DataLoader...")
        
        # Tá»‘i Æ°u bá»™ nhá»› trÆ°á»›c khi táº¡o DataLoader
        optimize_memory()
        
        # Sá»­ dá»¥ng pin_memory=False náº¿u khÃ´ng Ä‘á»§ RAM
        pin_memory = torch.cuda.is_available() and args.pin_memory
        persistent_workers = False if args.num_workers > 0 else False
        prefetch_factor = 2 if args.num_workers > 0 else None
        
        # Sá»­ dá»¥ng sampler cÃ¢n báº±ng náº¿u cáº§n
        if args.balance_classes:
            train_sampler = get_balanced_sampler(train_dataset, num_classes=2)
            train_loader = DataLoader(
                train_dataset, batch_size=args.batch_size, sampler=train_sampler,
                num_workers=args.num_workers, pin_memory=pin_memory,
                persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
                drop_last=False
            )
        else:
            train_loader = DataLoader(
                train_dataset, batch_size=args.batch_size, shuffle=True,
                num_workers=args.num_workers, pin_memory=pin_memory,
                persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
                drop_last=False
            )
        
        val_loader = DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=args.num_workers, pin_memory=pin_memory,
            persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
            drop_last=False
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=args.num_workers, pin_memory=pin_memory,
            persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
            drop_last=False
        )
        
        print(f"âœ… ÄÃ£ táº¡o DataLoader vá»›i batch_size={args.batch_size}, num_workers={args.num_workers}, pin_memory={pin_memory}")
        optimize_memory()  # Tá»‘i Æ°u bá»™ nhá»› sau khi táº¡o xong DataLoaders
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o DataLoader: {e}")
        print("  Thá»­ giáº£m batch_size hoáº·c num_workers Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›")
        return
    
    # Chá»n thiáº¿t bá»‹
    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    print(f"\nğŸ–¥ï¸ Sá»­ dá»¥ng thiáº¿t bá»‹: {device}")
    
    # Tá»‘i Æ°u hÃ³a bá»™ nhá»› trÆ°á»›c khi khá»Ÿi táº¡o mÃ´ hÃ¬nh
    optimize_memory()
    
    # Khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i kÃ­ch thÆ°á»›c tá»‘i Æ°u Ä‘á»ƒ Ä‘áº¡t 2.8M tham sá»‘
    try:
        # Cáº¥u hÃ¬nh Ä‘á»ƒ cÃ³ Ä‘Ãºng khoáº£ng 2.8M tham sá»‘ - Tá»‘i Æ°u cho PCC cao
        model = ConvNeXtTransformerLite(
            num_classes=2, 
            embed_dim=160,                    # Tá»‘i Æ°u cho PCC: giáº£m xuá»‘ng 160
            num_heads=5,                      # Tá»‘i Æ°u cho PCC: giáº£m xuá»‘ng 5
            num_transformer_layers=4,         # Tá»‘i Æ°u cho PCC: giáº£m xuá»‘ng 4 layers
            dropout=args.dropout,             # Dropout chÃ­nh
            dropout_path=0.05                 # Giáº£m dropout_path Ä‘á»ƒ cáº£i thiá»‡n PCC
        ).to(device)
        total_params = count_parameters(model)
        print(f"âœ… Khá»Ÿi táº¡o mÃ´ hÃ¬nh {model.__class__.__name__} thÃ nh cÃ´ng")
        print(f"ğŸ“Š MÃ´ hÃ¬nh cÃ³ tá»•ng cá»™ng {total_params:,} tham sá»‘ ({total_params/1e6:.2f}M)")
        print(f"  Embed dim: 160, Num heads: 5, Transformer layers: 4 (Tá»‘i Æ°u cho PCC)")
        print(f"  Dropout: {args.dropout}, Dropout path: 0.05, Weight decay: {args.weight_decay}")
    except Exception as e:
        print(f"âŒ Lá»—i khi khá»Ÿi táº¡o mÃ´ hÃ¬nh: {e}")
        return
    
    # Huáº¥n luyá»‡n hoáº·c táº£i mÃ´ hÃ¬nh
    try:
        if not args.eval_only:
            print(f"\nğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i {args.epochs} epochs...")
            model, best_val_f1 = train_model(
                model, train_loader, val_loader,
                epochs=args.epochs,
                lr=args.learning_rate,
                device=device,
                use_amp=args.use_amp,
                use_mixup=args.use_mixup,
                use_swa=args.use_swa,
                weight_decay=args.weight_decay,
                use_early_stopping=args.use_early_stopping,
                patience=args.patience
            )
            print(f"\nâœ… Huáº¥n luyá»‡n hoÃ n táº¥t. F1 tá»‘t nháº¥t: {best_val_f1:.4f}")
        else:
            # Táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
            checkpoint_path = os.path.join(project_dir, 'checkpoints', f'{model.__class__.__name__}_independent_best.pth')
            if os.path.exists(checkpoint_path):
                model.load_state_dict(torch.load(checkpoint_path, map_location=device))
                print(f"âœ… ÄÃ£ táº£i mÃ´ hÃ¬nh tá»« {checkpoint_path}")
            else:
                print(f"âŒ KhÃ´ng tÃ¬m tháº¥y checkpoint táº¡i {checkpoint_path}")
                return
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n/táº£i mÃ´ hÃ¬nh: {e}")
        return
    
    # Táº¡o 2 file CSV riÃªng biá»‡t vÃ  so sÃ¡nh Ä‘á»ƒ cáº£i thiá»‡n PCC
    try:
        print("\nğŸ“Š Táº¡o file CSV dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh vÃ  AHI PSG...")
        
        # Äáº£m báº£o thÆ° má»¥c results tá»“n táº¡i
        results_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. Táº¡o file CSV dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh
        model_csv_path = os.path.join(results_dir, 'model_predictions_independent.csv')
        model_df = create_model_predictions_csv(model, datasets, patient_ids, device, model_csv_path)
        
        # 2. Táº¡o file CSV AHI PSG (ground truth)
        psg_csv_path = os.path.join(results_dir, 'ahi_psg_independent.csv')
        psg_df = create_ahi_psg_csv(datasets, patient_ids, psg_csv_path)
        
        # 3. So sÃ¡nh 2 file CSV Ä‘á»ƒ tÃ­nh MAE, RMSE, PCC
        comparison_csv_path = os.path.join(results_dir, 'comparison_results_independent.csv')
        comparison_df, metrics = compare_model_vs_psg(model_csv_path, psg_csv_path, comparison_csv_path)
        
        if comparison_df is not None:
            print(f"\nğŸ¯ SUMMARY METRICS (Independent Subject):")
            print(f"  ğŸ“ˆ MAE: {metrics['mae']:.2f}")
            print(f"  ğŸ“ˆ RMSE: {metrics['rmse']:.2f}")  
            print(f"  ğŸ“ˆ PCC: {metrics['pcc']:.4f}")
            print(f"  ğŸ“ˆ RÂ²: {metrics['r2']:.4f}")
            print(f"  ğŸ“ˆ Severity Accuracy: {metrics['severity_acc']:.2f}")
            
            print(f"\nğŸ“ FILES ÄÆ¯á»¢C Táº O:")
            print(f"  ğŸ“„ Dá»± Ä‘oÃ¡n mÃ´ hÃ¬nh: {model_csv_path}")
            print(f"  ğŸ“„ AHI PSG: {psg_csv_path}")
            print(f"  ğŸ“„ So sÃ¡nh chi tiáº¿t: {comparison_csv_path}")
        
        print("\nâœ… HoÃ n táº¥t Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh Independent Subject!")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o CSV vÃ  so sÃ¡nh: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    try:
        # Cháº¿ Ä‘á»™ Ä‘Æ¡n giáº£n giá»‘ng LSTM
        if len(sys.argv) == 1 or '--simple' in sys.argv:
            main_simple()
        else:
            # Cháº¿ Ä‘á»™ nÃ¢ng cao vá»›i argparse
            print("\nğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh huáº¥n luyá»‡n mÃ´ hÃ¬nh ConvNeXtTransformerLite (Independent Subject)...")
            parser = argparse.ArgumentParser(description='Huáº¥n luyá»‡n mÃ´ hÃ¬nh ConvNeXtTransformerLite cho phÃ¡t hiá»‡n ngÆ°ng thá»Ÿ khi ngá»§')
            parser.add_argument('--data_dir', type=str, help='ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c dá»¯ liá»‡u blocks')
            parser.add_argument('--batch_size', type=int, default=16, help='KÃ­ch thÆ°á»›c batch')
            parser.add_argument('--epochs', type=int, default=50, help='Sá»‘ epochs huáº¥n luyá»‡n')
            parser.add_argument('--learning_rate', type=float, default=2e-5, help='Tá»‘c Ä‘á»™ há»c tá»‘i Æ°u cho PCC cao')
            parser.add_argument('--weight_decay', type=float, default=0.005, help='Weight decay nháº¹ hÆ¡n cho PCC tá»‘t')
            parser.add_argument('--device', type=str, default='cuda', help='Thiáº¿t bá»‹ huáº¥n luyá»‡n (cuda/cpu)')
            parser.add_argument('--num_workers', type=int, default=0, help='Sá»‘ workers cho DataLoader')
            parser.add_argument('--pin_memory', action='store_true', help='Sá»­ dá»¥ng pin_memory cho DataLoader')
            parser.add_argument('--eval_only', action='store_true', help='Chá»‰ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh, khÃ´ng huáº¥n luyá»‡n')
            parser.add_argument('--use_amp', action='store_true', help='Sá»­ dá»¥ng Automatic Mixed Precision')
            parser.add_argument('--use_mixup', action='store_true', help='Sá»­ dá»¥ng ká»¹ thuáº­t MixUp')
            parser.add_argument('--mixup_alpha', type=float, default=0.05, help='MixUp alpha nháº¹ hÆ¡n cho PCC tá»‘t')
            parser.add_argument('--balance_classes', action='store_true', help='CÃ¢n báº±ng lá»›p báº±ng WeightedRandomSampler')
            parser.add_argument('--use_swa', action='store_true', help='Sá»­ dá»¥ng Stochastic Weight Averaging')
            parser.add_argument('--dropout', type=float, default=0.08, help='Dropout tháº¥p hÆ¡n cho PCC tá»‘t')
            parser.add_argument('--use_early_stopping', action='store_true', help='Sá»­ dá»¥ng early stopping')
            parser.add_argument('--patience', type=int, default=8, help='Sá»‘ epochs chá» Ä‘á»£i cáº£i thiá»‡n trÆ°á»›c khi dá»«ng')
            
            args = parser.parse_args()
            
            # Äáº·t seed cho tÃ¡i táº¡o káº¿t quáº£
            set_seed(42)
            
            main()
        
        print("\nâœ… ChÆ°Æ¡ng trÃ¬nh hoÃ n táº¥t!")
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        print("\nğŸ” Chi tiáº¿t lá»—i:")
        traceback.print_exc()
