"""
Script hu·∫•n luy·ªán m√¥ h√¨nh ConvNeXtTransformerLite v√† t√≠nh to√°n ch·ªâ s·ªë AHI
theo ph∆∞∆°ng ph√°p Dependent Subject v·ªõi c√°c k·ªπ thu·∫≠t ti√™n ti·∫øn
"""
import os
import sys
import gc
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import traceback
from datetime import datetime
from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler
from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, 
                           confusion_matrix, mean_absolute_error, mean_squared_error, r2_score)
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c g·ªëc
project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_dir not in sys.path:
    sys.path.append(project_dir)

# Import t·ª´ utils
try:
    from utils.data_splitting import dependent_subject_split
    from utils.metrics import calculate_ahi_from_predictions, classify_osa_severity
    from utils.visualization import plot_confusion_matrix
except ImportError:
    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y utils modules, s·∫Ω t·∫°o c√°c h√†m thay th·∫ø...")

# Import c√°c module
from models.convnext_transformer_lite import ConvNeXtTransformerLite
try:
    from dataset.lazy_apnea_dataset import LazyApneaSingleDataset as LazyApneaDataset
    from dataset.lazy_apnea_dataset import MixUpDataset
except ImportError:
    from dataset.lazy_apnea_dataset import LazyApneaSequenceDataset as LazyApneaDataset


def evaluate(model, dataloader, device, name=""):
    """ƒê√°nh gi√° m√¥ h√¨nh binary classification - format gi·ªëng LSTM"""
    model.eval()
    all_preds, all_labels = [], []
    
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device).float()
            
            # Ensure y is the right shape for binary classification
            if y.dim() > 1:
                y = y.squeeze()
            
            out = model(x).squeeze()
            
            # Convert to binary predictions (0 or 1)
            binary_preds = (torch.sigmoid(out) > 0.5).cpu().numpy().astype(int)
            all_preds.extend(binary_preds)
            all_labels.extend(y.cpu().numpy().astype(int))

    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    
    print(f"üìä {name} - Acc: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
    return acc, f1


def count_parameters(model):
    """ƒê·∫øm t·ªïng s·ªë tham s·ªë c√≥ th·ªÉ hu·∫•n luy·ªán c·ªßa m√¥ h√¨nh"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def train_simple(model, train_loader, val_loader, test_loader, device, epochs=30, lr=5e-5, resume_path=None):
    """Hu·∫•n luy·ªán m√¥ h√¨nh binary classification - format gi·ªëng LSTM"""
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.008)
    criterion = nn.BCEWithLogitsLoss()  # Binary classification loss

    best_f1 = 0
    start_epoch = 0
    patience = 10
    patience_counter = 0

    # Resume t·ª´ checkpoint n·∫øu c√≥
    if resume_path and os.path.exists(resume_path):
        print(f"üîÑ Resume from checkpoint: {resume_path}")
        try:
            checkpoint = torch.load(resume_path, map_location=device)
            if isinstance(checkpoint, dict):
                model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))
                if 'optimizer_state_dict' in checkpoint:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                start_epoch = checkpoint.get('epoch', 0)
                best_f1 = checkpoint.get('best_f1', 0)
            else:
                model.load_state_dict(checkpoint)
            print(f"‚úÖ Loaded checkpoint with F1: {best_f1:.4f}")
        except Exception as e:
            print(f"‚ö†Ô∏è Cannot load checkpoint: {e}")

    for epoch in range(start_epoch, epochs):
        model.train()
        all_preds, all_labels = [], []
        total_loss = 0

        print(f"\nüîÅ Epoch {epoch + 1}/{epochs} - Training...")
        for i, (x, y) in tqdm(enumerate(train_loader), total=len(train_loader)):
            x, y = x.to(device), y.to(device).float()  # Convert to float for BCE loss
            
            # Ensure y is the right shape for binary classification
            if y.dim() > 1:
                y = y.squeeze()
            
            optimizer.zero_grad()

            out = model(x).squeeze()  # Remove extra dimensions for binary output
            loss = criterion(out, y)

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            # Convert to binary predictions (0 or 1)
            binary_preds = (torch.sigmoid(out) > 0.5).cpu().numpy().astype(int)
            all_preds.extend(binary_preds)
            all_labels.extend(y.cpu().numpy().astype(int))

        train_f1 = f1_score(all_labels, all_preds)
        train_acc = accuracy_score(all_labels, all_preds)
        val_acc, val_f1 = evaluate(model, val_loader, device, name="Validation")

        print(f"üìä Epoch {epoch+1}: Train F1={train_f1:.4f}, Train Acc={train_acc:.4f}")
        print(f"          Val F1={val_f1:.4f}, Val Acc={val_acc:.4f}")

        # L∆∞u checkpoint t·ªët nh·∫•t
        if val_f1 > best_f1:
            best_f1 = val_f1
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_f1': best_f1,
                'val_acc': val_acc,
                'train_acc': train_acc,
                'train_f1': train_f1
            }
            torch.save(checkpoint, "checkpoints/ConvNeXtTransformerLite_best_f1.pth")
            print(f"üíæ Saved best model with F1: {best_f1:.4f}")
        else:
            patience_counter += 1

        # L∆∞u ƒë·ªãnh k·ª≥ m·ªói 10 epoch
        if (epoch + 1) % 10 == 0:
            checkpoint = {
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_f1': best_f1,
                'val_acc': val_acc
            }
            torch.save(checkpoint, f"checkpoints/transformer_epoch_{epoch+1}.pth")

        print(f"[Epoch {epoch + 1}] Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, "
              f"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}")

        # Early stopping
        if patience_counter >= patience:
            print(f"‚èπÔ∏è Early stopping triggered after {patience} epochs without improvement")
            break

    print(f"\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t. Best Val F1: {best_f1:.4f}")

    # ƒê√°nh gi√° tr√™n t·∫≠p test v·ªõi model t·ªët nh·∫•t
    best_checkpoint = torch.load("checkpoints/ConvNeXtTransformerLite_best_f1.pth")
    model.load_state_dict(best_checkpoint['model_state_dict'])
    test_acc, test_f1 = evaluate(model, test_loader, device, name="Testing")
    
    print(f"\nüìà Final Results:")
    print(f"   - Best Validation Acc: {best_checkpoint.get('val_acc', 0):.4f}")
    print(f"   - Best Validation F1: {best_f1:.4f}")
    print(f"   - Test Acc: {test_acc:.4f}")
    print(f"   - Test F1: {test_f1:.4f}")

    return model, best_f1


def load_data(data_root, seq_len=5, batch_size=48):
    """Load d·ªØ li·ªáu - format gi·ªëng LSTM"""
    patients = sorted(os.listdir(data_root))
    datasets = []

    print(f"üìÇ ƒêang load d·ªØ li·ªáu t·ª´ {data_root}...")
    for p in patients:
        p_dir = os.path.join(data_root, p)
        if os.path.isdir(p_dir):
            try:
                print(f"üì• Loading block: {p}")
                ds = LazyApneaDataset(p_dir)
                print(f"‚úÖ Loaded {p} - T·ªïng sequence: {len(ds)}")
                datasets.append(ds)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói v·ªõi {p}: {e}")

    if not datasets:
        raise RuntimeError("‚ùå Kh√¥ng c√≥ block n√†o ƒë∆∞·ª£c load!")

    full_dataset = ConcatDataset(datasets)
    total_len = len(full_dataset)
    print(f"üìä T·ªïng s·ªë sequence: {total_len}")

    train_len = int(0.8 * total_len)
    val_len = int(0.1 * total_len)
    test_len = total_len - train_len - val_len

    train_ds, val_ds, test_ds = torch.utils.data.random_split(full_dataset, [train_len, val_len, test_len])

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)

    return train_loader, val_loader, test_loader


def predict_and_save_csv_per_block(model, data_root, device, seq_len=5):
    """T·∫°o CSV predictions - format gi·ªëng LSTM"""
    model.eval()
    os.makedirs("predictions", exist_ok=True)
    patients = sorted(os.listdir(data_root))

    print(f"\nüß™ L∆∞u d·ª± ƒëo√°n nh·ªã ph√¢n d∆∞·ªõi d·∫°ng CSV cho t·ª´ng block...")
    for p in patients:
        p_dir = os.path.join(data_root, p)
        if not os.path.isdir(p_dir):
            continue

        try:
            ds = LazyApneaDataset(p_dir)
            loader = DataLoader(ds, batch_size=48, shuffle=False)

            all_preds, all_labels = [], []
            with torch.no_grad():
                for x, y in loader:
                    x = x.to(device)
                    out = model(x)
                    preds = torch.argmax(out, dim=1)

                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(y.cpu().numpy())

            df = pd.DataFrame({
                "label_true": all_labels,
                "label_pred": all_preds,
            })
            df["correct"] = df["label_true"] == df["label_pred"]
            accuracy = df["correct"].mean()
            
            save_path = f"predictions/{p}_preds.csv"
            df.to_csv(save_path, index=False)
            print(f"‚úÖ ƒê√£ l∆∞u: {save_path} (Accuracy: {accuracy:.4f})")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói v·ªõi block {p}: {e}")


def main_simple():
    """H√†m main ƒë∆°n gi·∫£n gi·ªëng LSTM"""
    print("üöÄ ConvNeXt+Transformer Training v·ªõi Binary Predictions (Dependent)")
    print("üìã Workflow: Train ‚Üí Binary (0,1) ‚Üí 2 CSV files ‚Üí MAE/RMSE/PCC")
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n project_dir t·ª´ bi·∫øn global ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ƒë·∫ßu file
    data_path = os.path.join(project_dir, "data", "blocks")

    if not os.path.exists(data_path):
        raise RuntimeError(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {data_path}")

    os.makedirs("checkpoints", exist_ok=True)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"üîß Device: {device}")

    # Load data v·ªõi Dependent Subject approach (shuffle t·∫•t c·∫£ patients)
    train_loader, val_loader, test_loader = load_data(data_path, seq_len=5, batch_size=32)
    
    # Kh·ªüi t·∫°o model v·ªõi parameters t·ªëi ∆∞u - QUAN TR·ªåNG: num_classes=1 cho binary
    model = ConvNeXtTransformerLite(
        num_classes=1,     # 1 output cho binary classification
        embed_dim=160,
        num_heads=5,
        num_transformer_layers=4,
        dropout=0.08
    )
    
    # Model summary
    total_params = count_parameters(model)
    print(f"üèóÔ∏è Model: {total_params:,} parameters ({total_params/1e6:.2f}M)")
    print(f"  Architecture: ConvNeXt + Transformer ‚Üí Binary output (0,1)")

    # STEP 1: Training
    print("\nüöÄ STEP 1: Training model...")
    resume_ckpt = "checkpoints/ConvNeXtTransformerLite_best_f1.pth"
    model, best_f1 = train_simple(model, train_loader, val_loader, test_loader, device, epochs=30, lr=5e-5, resume_path=resume_ckpt)

    # T·∫°o full dataset cho predictions
    from dataset.lazy_apnea_dataset import LazyApneaDataset
    full_dataset = LazyApneaDataset(data_path, seq_len=5, use_cache=True)
    
    # STEP 2: T·∫°o 2 file CSV t·ª´ binary predictions
    print("\nüìä STEP 2: T·∫°o 2 CSV files t·ª´ binary predictions...")
    
    # File 1: Model binary predictions ‚Üí AHI
    model_df, model_csv = create_model_predictions_csv(model, full_dataset, device)
    
    # File 2: True labels ‚Üí True AHI
    psg_df, psg_csv = create_ahi_psg_csv(full_dataset)
    
    # STEP 3: So s√°nh 2 files v√† t√≠nh MAE, RMSE, PCC
    comparison_df, final_metrics = compare_files_and_calculate_metrics(model_csv, psg_csv)
    
    print("\n‚úÖ HO√ÄN TH√ÄNH! Binary predictions workflow (Dependent)!")
    print("üéØ Train ‚Üí Binary (0,1) ‚Üí CSV files ‚Üí MAE/RMSE/PCC calculation")
    if final_metrics:
        print(f"üèÜ Final PCC: {final_metrics['pcc']:.4f}")
    
    optimize_memory()
    """Gi·∫£i ph√≥ng b·ªô nh·ªõ cache v√† thu gom r√°c"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # Th√™m gi·∫£i ph√≥ng b·ªô nh·ªõ CUDA kh√¥ng s·ª≠ d·ª•ng
    torch.cuda.empty_cache()
    
    # ƒê·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ gi·ªõi h·∫°n cache PyTorch
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'


def mixup_data(x, y, alpha=0.3, device='cuda'):
    """Th·ª±c hi·ªán MixUp tr√™n batch d·ªØ li·ªáu"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(device)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam


def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """T√≠nh loss v·ªõi MixUp"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


def get_balanced_sampler(dataset, num_classes=2):
    """
    T·∫°o m·ªôt sampler c√¢n b·∫±ng c√°c l·ªõp trong t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán.
    """
    # L·∫•y t·∫•t c·∫£ c√°c nh√£n t·ª´ dataset
    labels = []
    if isinstance(dataset, torch.utils.data.ConcatDataset):
        for ds in dataset.datasets:
            for _, y in ds:
                labels.append(y.item())
    else:
        for _, y in dataset:
            labels.append(y.item())
    
    # ƒê·∫øm s·ªë l∆∞·ª£ng m·∫´u m·ªói l·ªõp
    labels = np.array(labels)
    class_counts = [np.sum(labels == i) for i in range(num_classes)]
    num_samples = len(labels)
    
    # T√≠nh tr·ªçng s·ªë cho t·ª´ng m·∫´u
    weights = [0] * num_samples
    for idx, label in enumerate(labels):
        weights[idx] = 1.0 / class_counts[label]
    
    # T·∫°o WeightedRandomSampler
    weights = torch.DoubleTensor(weights)
    sampler = torch.utils.data.WeightedRandomSampler(
        weights, len(weights), replacement=True
    )
    
    return sampler


def set_seed(seed=42):
    """ƒê·∫∑t seed cho t√°i t·∫°o k·∫øt qu·∫£"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def calculate_ahi_from_predictions(true_labels, pred_labels, epoch_duration_seconds=30):
    """T√≠nh AHI t·ª´ d·ª± ƒëo√°n"""
    # T√≠nh th·ªùi gian ng·ªß t·ªïng c·ªông (gi·ªù)
    total_time_hours = (len(true_labels) * epoch_duration_seconds) / 3600
    
    # ƒê·∫øm s·ªë s·ª± ki·ªán ng∆∞ng th·ªü
    true_apnea_events = np.sum(true_labels == 1)
    pred_apnea_events = np.sum(pred_labels == 1)
    
    # T√≠nh AHI
    true_ahi = true_apnea_events / total_time_hours if total_time_hours > 0 else 0
    pred_ahi = pred_apnea_events / total_time_hours if total_time_hours > 0 else 0
    
    # T√≠nh c√°c metrics kh√°c
    mae = mean_absolute_error([true_ahi], [pred_ahi])
    rmse = np.sqrt(mean_squared_error([true_ahi], [pred_ahi]))
    
    metrics = {
        'mae': mae,
        'rmse': rmse,
        'true_events': true_apnea_events,
        'pred_events': pred_apnea_events,
        'total_time_hours': total_time_hours
    }
    
    return true_ahi, pred_ahi, metrics


# Helper functions cho binary prediction workflow
def calculate_ahi_from_binary_predictions(binary_array, epoch_duration_seconds=30):
    """
    T√≠nh AHI t·ª´ chu·ªói binary predictions
    AHI = (s·ªë epoch c√≥ apnea * epochs_per_hour) / total_hours
    """
    if len(binary_array) == 0:
        return 0.0
    
    epochs_per_hour = 3600 / epoch_duration_seconds  # 120 epochs/hour v·ªõi 30s/epoch
    total_hours = len(binary_array) / epochs_per_hour
    apnea_events = np.sum(binary_array == 1)
    
    if total_hours <= 0:
        return 0.0
    
    ahi = apnea_events / total_hours
    return float(ahi)

def classify_osa_severity(ahi):
    """Ph√¢n lo·∫°i m·ª©c ƒë·ªô nghi√™m tr·ªçng OSA"""
    if ahi < 5:
        return 'Normal'
    elif ahi < 15:
        return 'Mild'
    elif ahi < 30:
        return 'Moderate'
    else:
        return 'Severe'


def dependent_subject_split_optimized(datasets, patient_ids, train_ratio=0.8, seed=42):
    """
    Chia d·ªØ li·ªáu theo ph∆∞∆°ng ph√°p dependent subject t·ªëi ∆∞u v·ªõi t·ª∑ l·ªá 80/10/10
    M·ªói b·ªánh nh√¢n ƒë∆∞·ª£c chia th√†nh train (80%), validation (10%) v√† test (10%)
    
    Args:
        datasets: List c√°c dataset c·ªßa t·ª´ng b·ªánh nh√¢n
        patient_ids: List ID b·ªánh nh√¢n
        train_ratio: T·ª∑ l·ªá d·ªØ li·ªáu train (0.8 = 80%)
        seed: Seed cho random
        
    Returns:
        train_datasets, val_datasets, test_datasets: List c√°c dataset cho m·ªói t·∫≠p
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    train_datasets = []
    val_datasets = []
    test_datasets = []
    
    print("üîÄ Chia d·ªØ li·ªáu theo dependent subject t·ªëi ∆∞u (80/10/10)...")
    
    total_train_samples = 0
    total_val_samples = 0
    total_test_samples = 0
    
    for dataset, patient_id in zip(datasets, patient_ids):
        try:
            total_samples = len(dataset)
            
            # T·∫°o indices v√† shuffle
            indices = list(range(total_samples))
            random.shuffle(indices)
            
            # Chia theo t·ª∑ l·ªá 80/10/10
            train_size = int(train_ratio * total_samples)
            remaining_size = total_samples - train_size
            
            # Chia ph·∫ßn remaining th√†nh val v√† test (m·ªói ph·∫ßn 10%)
            val_size = remaining_size // 2
            test_size = remaining_size - val_size
            
            # T·∫°o indices cho t·ª´ng t·∫≠p
            train_indices = indices[:train_size]
            val_indices = indices[train_size:train_size + val_size]
            test_indices = indices[train_size + val_size:]
            
            # T·∫°o Subset
            train_subset = torch.utils.data.Subset(dataset, train_indices)
            val_subset = torch.utils.data.Subset(dataset, val_indices)
            test_subset = torch.utils.data.Subset(dataset, test_indices)
            
            train_datasets.append(train_subset)
            val_datasets.append(val_subset)
            test_datasets.append(test_subset)
            
            # C·∫≠p nh·∫≠t t·ªïng s·ªë m·∫´u
            total_train_samples += train_size
            total_val_samples += val_size
            total_test_samples += test_size
            
            # In th√¥ng tin chi ti·∫øt
            train_pct = (train_size / total_samples) * 100
            val_pct = (val_size / total_samples) * 100
            test_pct = (test_size / total_samples) * 100
            
            print(f"  {patient_id}: {train_size}/{total_samples} ({train_pct:.1f}%) train, "
                  f"{val_size}/{total_samples} ({val_pct:.1f}%) val, "
                  f"{test_size}/{total_samples} ({test_pct:.1f}%) test")
            
            # T·ªëi ∆∞u b·ªô nh·ªõ
            optimize_memory()
            
        except Exception as e:
            print(f"‚ùå L·ªói khi chia d·ªØ li·ªáu cho {patient_id}: {e}")
            continue
    
    print(f"\nüìä T·ªïng k·∫øt chia d·ªØ li·ªáu:")
    total_samples = total_train_samples + total_val_samples + total_test_samples
    print(f"  Train: {total_train_samples}/{total_samples} ({total_train_samples/total_samples*100:.1f}%)")
    print(f"  Validation: {total_val_samples}/{total_samples} ({total_val_samples/total_samples*100:.1f}%)")
    print(f"  Test: {total_test_samples}/{total_samples} ({total_test_samples/total_samples*100:.1f}%)")
    
    return train_datasets, val_datasets, test_datasets


def train_model(model, train_loader, val_loader, epochs=20, lr=1e-3, device='cuda', 
                use_amp=True, use_mixup=False, use_swa=False, weight_decay=0.01,
                use_early_stopping=False, patience=5):
    """Hu·∫•n luy·ªán m√¥ h√¨nh ƒë∆°n gi·∫£n h√≥a"""
    print(f"üöÄ Hu·∫•n luy·ªán m√¥ h√¨nh {model.__class__.__name__} tr√™n {device}")
    
    # T·∫°o th∆∞ m·ª•c checkpoints
    checkpoint_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, f'{model.__class__.__name__}_best.pth')
    
    # Kh·ªüi t·∫°o optimizer v√† criterion
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # S·ª≠ d·ª•ng weighted loss n·∫øu c·∫ßn
    if hasattr(train_loader.dataset, 'get_class_weights'):
        class_weights = train_loader.dataset.get_class_weights()
        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        print(f"  S·ª≠ d·ª•ng weighted loss v·ªõi weights: {class_weights}")
    else:
        criterion = nn.CrossEntropyLoss()
    
    # Learning rate scheduler - OneCycleLR v·ªõi cosine annealing
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, epochs=epochs, steps_per_epoch=len(train_loader),
        pct_start=0.3, div_factor=10, final_div_factor=100, anneal_strategy='cos'
    )
    
    # GradScaler cho mixed precision
    scaler = GradScaler() if use_amp else None
    
    # Stochastic Weight Averaging
    if use_swa:
        swa_model = torch.optim.swa_utils.AveragedModel(model)
        swa_start = epochs // 3  # B·∫Øt ƒë·∫ßu SWA ·ªü 1/3 qu√° tr√¨nh
    
    best_val_f1 = 0
    no_improve_epochs = 0
    
    # T·ªëi ∆∞u b·ªô nh·ªõ tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
    optimize_memory()
    
    for epoch in range(epochs):
        # TRAIN
        model.train()
        train_loss = 0
        all_preds, all_labels = [], []
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
        batch_count = 0
        
        for x, y in pbar:
            # Move data to device
            x, y = x.to(device), y.to(device)
            
            # Forward pass
            optimizer.zero_grad(set_to_none=True)  # Ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n
            
            if use_mixup:
                # √Åp d·ª•ng MixUp
                mixed_x, y_a, y_b, lam = mixup_data(x, y, alpha=args.mixup_alpha, device=device)
                
                if use_amp:
                    with autocast():
                        outputs = model(mixed_x)
                        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)
                    
                    # Backward and optimize
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(mixed_x)
                    loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)
                    
                    # Backward and optimize
                    loss.backward()
                    optimizer.step()
                
                # L·∫•y d·ª± ƒëo√°n cho metrics (t·ª´ d·ªØ li·ªáu g·ªëc)
                with torch.no_grad():
                    orig_outputs = model(x)
                    preds = orig_outputs.argmax(1).cpu().numpy()
            else:
                if use_amp:
                    with autocast():
                        outputs = model(x)
                        loss = criterion(outputs, y)
                    
                    # Backward and optimize
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(x)
                    loss = criterion(outputs, y)
                    
                    # Backward and optimize
                    loss.backward()
                    optimizer.step()
                
                preds = outputs.argmax(1).cpu().numpy()
            
            # Update scheduler
            scheduler.step()
            
            # Update metrics
            train_loss += loss.item()
            all_preds.extend(preds)
            all_labels.extend(y.cpu().numpy())
            
            # Update progress bar
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
            
            # X√≥a bi·∫øn t·∫°m ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ
            del x, y, outputs, loss, preds
            
            # T·ªëi ∆∞u b·ªô nh·ªõ ƒë·ªãnh k·ª≥ ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ
            batch_count += 1
            if batch_count % 20 == 0:  # C·ª© m·ªói 20 batch
                optimize_memory()
        
        # Calculate train metrics
        train_loss /= len(train_loader)
        train_acc = accuracy_score(all_labels, all_preds)
        train_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ sau khi t√≠nh to√°n xong metrics
        del all_preds, all_labels
        optimize_memory()
        
        # Update SWA if enabled
        if use_swa and epoch >= swa_start:
            swa_model.update_parameters(model)
        
        # VALIDATION
        model.eval()
        val_loss = 0
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for x, y in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]"):
                x, y = x.to(device), y.to(device)
                outputs = model(x)
                loss = criterion(outputs, y)
                
                val_loss += loss.item()
                preds = outputs.argmax(1).cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(y.cpu().numpy())
                
                # X√≥a bi·∫øn t·∫°m ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ
                del x, y, outputs, loss, preds
        
        # Calculate validation metrics
        val_loss /= len(val_loader)
        val_acc = accuracy_score(all_labels, all_preds)
        val_f1 = f1_score(all_labels, all_preds, average='weighted')
        
        # Gi·∫£i ph√≥ng b·ªô nh·ªõ sau khi t√≠nh to√°n xong metrics
        del all_preds, all_labels
        optimize_memory()
        
        # Save best model
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), checkpoint_path)
            print(f"‚úÖ L∆∞u m√¥ h√¨nh t·ªët nh·∫•t v·ªõi F1={val_f1:.4f}")
        
        # Print epoch results
        print(f"[Epoch {epoch+1}/{epochs}] "
              f"Train Loss: {train_loss:.4f}, F1: {train_f1:.4f} | "
              f"Val Loss: {val_loss:.4f}, F1: {val_f1:.4f}")
        
        # Early stopping
        if use_early_stopping:
            if val_f1 > best_val_f1:
                no_improve_epochs = 0
            else:
                no_improve_epochs += 1
                print(f"  Kh√¥ng c·∫£i thi·ªán F1 ({no_improve_epochs}/{patience} epochs)")
                
            if no_improve_epochs >= patience:
                print(f"‚ö†Ô∏è Early stopping t·∫°i epoch {epoch+1}/{epochs}")
                break
        
        # T·ªëi ∆∞u b·ªô nh·ªõ ·ªü cu·ªëi m·ªói epoch
        optimize_memory()
    
    # T·ªëi ∆∞u b·ªô nh·ªõ tr∆∞·ªõc khi k·∫øt th√∫c
    optimize_memory()
    return model, best_val_f1


def create_model_predictions_csv(model, full_dataset, device):
    """
    B∆Ø·ªöC 2A: T·∫°o File 1 - Model binary predictions ‚Üí AHI cho t·ª´ng patient
    """
    print("üîç B∆Ø·ªöC 2A: T·∫°o File 1 - Model predictions...")
    model.eval()
    
    results_dir = os.path.join(project_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    
    predictions = []
    
    with torch.no_grad():
        for patient_id in tqdm(full_dataset.patient_ids[:25], desc="Processing patients"):
            try:
                patient_data = full_dataset.get_patient_data(patient_id, limit_blocks=None)
                
                if not patient_data:
                    continue
                
                # Process all blocks for this patient
                patient_preds = []
                for block_data in patient_data:
                    try:
                        if len(block_data) < 3:
                            continue
                            
                        features = block_data[0]
                        if features is None or features.size == 0:
                            continue
                        
                        # Convert to tensor v√† predict
                        features_tensor = torch.FloatTensor(features).unsqueeze(0).to(device)
                        
                        # Model prediction
                        with autocast():
                            outputs = model(features_tensor)
                            
                        # Convert to binary prediction (0 or 1)
                        pred_prob = torch.sigmoid(outputs.squeeze()).cpu().numpy()
                        binary_pred = 1 if pred_prob > 0.5 else 0
                        patient_preds.append(binary_pred)
                        
                    except Exception as e:
                        continue
                
                # Calculate AHI from binary predictions
                if len(patient_preds) > 0:
                    binary_array = np.array(patient_preds)
                    predicted_ahi = calculate_ahi_from_binary_predictions(binary_array)
                    predicted_severity = classify_osa_severity(predicted_ahi)
                    
                    predictions.append({
                        'patient_id': patient_id,
                        'predicted_ahi': predicted_ahi,
                        'predicted_severity': predicted_severity,
                        'total_epochs': len(patient_preds),
                        'apnea_events_pred': np.sum(binary_array == 1),
                        'normal_events_pred': np.sum(binary_array == 0),
                        'total_sleep_hours': len(patient_preds) * 30 / 3600
                    })
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error processing {patient_id}: {e}")
    
    # Save FILE 1
    predictions_df = pd.DataFrame(predictions)
    predictions_csv = os.path.join(results_dir, 'model_predictions_dependent.csv')
    predictions_df.to_csv(predictions_csv, index=False)
    
    print(f"‚úÖ FILE 1 saved: {predictions_csv}")
    print(f"  Format: Binary predictions ‚Üí Predicted AHI cho {len(predictions)} patients")
    return predictions_df, predictions_csv

def create_ahi_psg_csv(full_dataset):
    """
    B∆Ø·ªöC 2B: T·∫°o File 2 - True labels ‚Üí True AHI cho t·ª´ng patient  
    """
    print("üîç B∆Ø·ªöC 2B: T·∫°o File 2 - True AHI PSG...")
    
    results_dir = os.path.join(project_dir, 'results')
    os.makedirs(results_dir, exist_ok=True)
    
    psg_results = []
    
    for patient_id in tqdm(full_dataset.patient_ids[:25], desc="Processing PSG"):
        try:
            patient_data = full_dataset.get_patient_data(patient_id, limit_blocks=None)
            
            if not patient_data:
                continue
            
            # Collect true labels cho patient n√†y
            true_labels = []
            for block_data in patient_data:
                try:
                    if len(block_data) < 3:
                        continue
                        
                    label = block_data[1]  # True label
                    if label is not None:
                        true_labels.append(int(label))
                        
                except Exception as e:
                    continue
            
            if len(true_labels) > 0:
                # Convert true labels ‚Üí true AHI
                true_array = np.array(true_labels)
                true_ahi = calculate_ahi_from_binary_predictions(true_array)
                true_severity = classify_osa_severity(true_ahi)
                
                psg_results.append({
                    'patient_id': patient_id,
                    'ahi_psg': true_ahi,
                    'severity_psg': true_severity,
                    'total_epochs': len(true_labels),
                    'apnea_events_true': np.sum(true_array == 1),
                    'normal_events_true': np.sum(true_array == 0),
                    'total_sleep_hours': len(true_labels) * 30 / 3600
                })
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {patient_id}: {e}")
    
    # Save FILE 2
    psg_df = pd.DataFrame(psg_results)
    psg_csv = os.path.join(results_dir, 'ahi_psg_dependent.csv')
    psg_df.to_csv(psg_csv, index=False)
    
    print(f"‚úÖ FILE 2 saved: {psg_csv}")
    print(f"  Format: True labels ‚Üí True AHI cho {len(psg_results)} patients")
    return psg_df, psg_csv

def compare_files_and_calculate_metrics(model_csv, psg_csv):
    """
    B∆Ø·ªöC 3: ƒê·ªçc 2 file CSV v√† t√≠nh MAE, RMSE, PCC
    """
    print("üîç B∆Ø·ªöC 3: So s√°nh 2 files v√† t√≠nh MAE, RMSE, PCC...")
    
    # ƒê·ªçc 2 files
    model_df = pd.read_csv(model_csv)
    psg_df = pd.read_csv(psg_csv)
    
    # Merge theo patient_id
    merged_df = pd.merge(model_df, psg_df, on='patient_id', how='inner')
    
    if len(merged_df) == 0:
        print("‚ùå Kh√¥ng c√≥ patient n√†o match")
        return None, None
    
    print(f"‚úÖ Matched {len(merged_df)} patients")
    
    # Extract AHI values
    predicted_ahi = merged_df['predicted_ahi'].values
    true_ahi = merged_df['ahi_psg'].values
    
    # Calculate metrics
    mae = mean_absolute_error(true_ahi, predicted_ahi)
    rmse = np.sqrt(mean_squared_error(true_ahi, predicted_ahi))
    
    # PCC (Pearson Correlation Coefficient)
    correlation_matrix = np.corrcoef(predicted_ahi, true_ahi)
    pcc = correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0.0
    
    # Create comparison DataFrame
    comparison_df = merged_df[['patient_id', 'predicted_ahi', 'ahi_psg']].copy()
    comparison_df['absolute_error'] = np.abs(predicted_ahi - true_ahi)
    comparison_df['squared_error'] = (predicted_ahi - true_ahi) ** 2
    
    # Save comparison file
    results_dir = os.path.join(project_dir, 'results')
    comparison_csv = os.path.join(results_dir, 'ahi_comparison_dependent.csv')
    comparison_df.to_csv(comparison_csv, index=False)
    
    # Final metrics
    final_metrics = {
        'mae': mae,
        'rmse': rmse,
        'pcc': pcc,
        'num_patients': len(merged_df)
    }
    
    print(f"üìä FINAL METRICS (Dependent Subject):")
    print(f"  üë• Patients: {final_metrics['num_patients']}")
    print(f"  üìâ MAE: {final_metrics['mae']:.4f}")
    print(f"  üìâ RMSE: {final_metrics['rmse']:.4f}")
    print(f"  üéØ PCC: {final_metrics['pcc']:.4f}")
    
    return comparison_df, final_metrics


def main():
    """H√†m ch√≠nh th·ª±c hi·ªán qu√° tr√¨nh hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh"""
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu
    project_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    
    if args.data_dir:
        if os.path.isabs(args.data_dir):
            data_dir = args.data_dir
        else:
            data_dir = os.path.abspath(os.path.join(os.getcwd(), args.data_dir))
            if not os.path.exists(data_dir):
                data_dir = os.path.abspath(os.path.join(project_dir, args.data_dir))
    else:
        data_dir = os.path.join(project_dir, "data", "blocks")
    
    print(f"üîç ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu: {data_dir}")
    
    # T·ªëi ∆∞u h√≥a b·ªô nh·ªõ tr∆∞·ªõc khi t·∫£i d·ªØ li·ªáu
    optimize_memory()
    
    # Ki·ªÉm tra t·ªìn t·∫°i c·ªßa th∆∞ m·ª•c d·ªØ li·ªáu
    if not os.path.exists(data_dir):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c blocks t·∫°i {data_dir}")
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c hi·ªán t·∫°i: {os.getcwd()}")
        print("‚ö†Ô∏è Vui l√≤ng ch·∫°y build_dataset.py tr∆∞·ªõc ho·∫∑c ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n")
        return
    
    # T·∫£i d·ªØ li·ªáu
    try:
        patient_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) 
                      if os.path.isdir(os.path.join(data_dir, d))]
        
        print(f"üìä ƒêang t·∫£i d·ªØ li·ªáu t·ª´ {len(patient_dirs)} b·ªánh nh√¢n")
        
        datasets = []
        patient_ids = []
        
        # S·∫Øp x·∫øp theo t√™n ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n
        patient_dirs.sort()
        
        for p_dir in patient_dirs:
            try:
                patient_id = os.path.basename(p_dir)
                print(f"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu t·ª´ {patient_id}...")
                
                # T·ªëi ∆∞u h√≥a b·ªô nh·ªõ tr∆∞·ªõc khi t·∫£i m·ªói b·ªánh nh√¢n
                optimize_memory()
                
                ds = LazyApneaDataset(p_dir)
                if len(ds) > 0:
                    datasets.append(ds)
                    patient_ids.append(patient_id)
                    print(f"‚úÖ ƒê√£ t·∫£i {len(ds)} m·∫´u t·ª´ {patient_id}")
                else:
                    print(f"‚ö†Ô∏è Kh√¥ng c√≥ m·∫´u n√†o t·ª´ {patient_id}")
                
                # Ki·ªÉm tra b·ªô nh·ªõ sau khi t·∫£i d·ªØ li·ªáu t·ª´ m·ªói b·ªánh nh√¢n
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # GB
                    print(f"  B·ªô nh·ªõ CUDA s·ª≠ d·ª•ng: {allocated:.2f} GB")
                
            except Exception as e:
                print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ {p_dir}: {e}")
                print(f"  Chi ti·∫øt: {traceback.format_exc()}")
                
        print(f"‚úÖ ƒê√£ t·∫£i d·ªØ li·ªáu t·ª´ {len(datasets)}/{len(patient_dirs)} b·ªánh nh√¢n")
        
        # T·ªëi ∆∞u h√≥a b·ªô nh·ªõ sau khi t·∫£i d·ªØ li·ªáu
        optimize_memory()
        
        if not datasets:
            print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán.")
            return
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        return
    
    # Chia d·ªØ li·ªáu theo ph∆∞∆°ng ph√°p dependent subject t·ªëi ∆∞u
    print("\nüîÄ ƒêang chia d·ªØ li·ªáu theo ph∆∞∆°ng ph√°p dependent subject t·ªëi ∆∞u...")
    try:
        # T·ªëi ∆∞u h√≥a b·ªô nh·ªõ tr∆∞·ªõc khi chia d·ªØ li·ªáu
        optimize_memory()
        
        # S·ª≠ d·ª•ng h√†m chia d·ªØ li·ªáu t·ªëi ∆∞u v·ªõi t·ª∑ l·ªá 80/10/10
        train_datasets, val_datasets, test_datasets = dependent_subject_split_optimized(
            datasets, patient_ids, train_ratio=0.8, seed=42
        )
        
        # T·∫°o combined datasets
        train_dataset = ConcatDataset(train_datasets)
        val_dataset = ConcatDataset(val_datasets)
        test_dataset = ConcatDataset(test_datasets)
        
        print(f"\nüìà T·ªïng s·ªë m·∫´u sau khi g·ªôp: {len(train_dataset)} train, {len(val_dataset)} validation, {len(test_dataset)} test")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi chia d·ªØ li·ªáu: {e}")
        return
    
    # T·∫°o DataLoader
    try:
        print("\nüîÑ ƒêang t·∫°o DataLoader...")
        
        # T·ªëi ∆∞u b·ªô nh·ªõ tr∆∞·ªõc khi t·∫°o DataLoader
        optimize_memory()
        
        # S·ª≠ d·ª•ng pin_memory=False n·∫øu kh√¥ng ƒë·ªß RAM
        pin_memory = torch.cuda.is_available() and args.pin_memory
        persistent_workers = False if args.num_workers > 0 else False
        prefetch_factor = 2 if args.num_workers > 0 else None
        
        # S·ª≠ d·ª•ng sampler c√¢n b·∫±ng n·∫øu c·∫ßn
        if args.balance_classes:
            train_sampler = get_balanced_sampler(train_dataset, num_classes=2)
            train_loader = DataLoader(
                train_dataset, batch_size=args.batch_size, sampler=train_sampler,
                num_workers=args.num_workers, pin_memory=pin_memory,
                persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
                drop_last=False
            )
        else:
            train_loader = DataLoader(
                train_dataset, batch_size=args.batch_size, shuffle=True,
                num_workers=args.num_workers, pin_memory=pin_memory,
                persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
                drop_last=False
            )
        
        val_loader = DataLoader(
            val_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=args.num_workers, pin_memory=pin_memory,
            persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
            drop_last=False
        )
        
        test_loader = DataLoader(
            test_dataset, batch_size=args.batch_size, shuffle=False,
            num_workers=args.num_workers, pin_memory=pin_memory,
            persistent_workers=persistent_workers, prefetch_factor=prefetch_factor,
            drop_last=False
        )
        
        print(f"‚úÖ ƒê√£ t·∫°o DataLoader v·ªõi batch_size={args.batch_size}, num_workers={args.num_workers}, pin_memory={pin_memory}")
        optimize_memory()  # T·ªëi ∆∞u b·ªô nh·ªõ sau khi t·∫°o xong DataLoaders
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o DataLoader: {e}")
        print("  Th·ª≠ gi·∫£m batch_size ho·∫∑c num_workers ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ")
        return
    
    # Ch·ªçn thi·∫øt b·ªã
    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    print(f"\nüñ•Ô∏è S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    
    # T·ªëi ∆∞u h√≥a b·ªô nh·ªõ tr∆∞·ªõc khi kh·ªüi t·∫°o m√¥ h√¨nh
    optimize_memory()
    
    # Kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi k√≠ch th∆∞·ªõc t·ªëi ∆∞u ƒë·ªÉ ƒë·∫°t 2.8M tham s·ªë
    try:
        # C·∫•u h√¨nh ƒë·ªÉ c√≥ ƒë√∫ng kho·∫£ng 2.8M tham s·ªë - T·ªëi ∆∞u cho PCC cao
        model = ConvNeXtTransformerLite(
            num_classes=2, 
            embed_dim=160,                    # T·ªëi ∆∞u cho PCC: gi·∫£m xu·ªëng 160
            num_heads=5,                      # T·ªëi ∆∞u cho PCC: gi·∫£m xu·ªëng 5
            num_transformer_layers=4,         # T·ªëi ∆∞u cho PCC: gi·∫£m xu·ªëng 4 layers
            dropout=args.dropout,             # Dropout ch√≠nh
            dropout_path=0.05                 # Gi·∫£m dropout_path ƒë·ªÉ c·∫£i thi·ªán PCC
        ).to(device)
        total_params = count_parameters(model)
        print(f"‚úÖ Kh·ªüi t·∫°o m√¥ h√¨nh {model.__class__.__name__} th√†nh c√¥ng")
        print(f"üìä M√¥ h√¨nh c√≥ t·ªïng c·ªông {total_params:,} tham s·ªë ({total_params/1e6:.2f}M)")
        print(f"  Embed dim: 160, Num heads: 5, Transformer layers: 4 (T·ªëi ∆∞u cho PCC)")
        print(f"  Dropout: {args.dropout}, Dropout path: 0.05, Weight decay: {args.weight_decay}")
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o m√¥ h√¨nh: {e}")
        return
    
    # Hu·∫•n luy·ªán ho·∫∑c t·∫£i m√¥ h√¨nh
    try:
        if not args.eval_only:
            print(f"\nüöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi {args.epochs} epochs...")
            model, best_val_f1 = train_model(
                model, train_loader, val_loader,
                epochs=args.epochs,
                lr=args.learning_rate,
                device=device,
                use_amp=args.use_amp,
                use_mixup=args.use_mixup,
                use_swa=args.use_swa,
                weight_decay=args.weight_decay,
                use_early_stopping=args.use_early_stopping,
                patience=args.patience
            )
            print(f"\n‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t. F1 t·ªët nh·∫•t: {best_val_f1:.4f}")
        else:
            # T·∫£i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
            checkpoint_path = os.path.join(project_dir, 'checkpoints', f'{model.__class__.__name__}_best.pth')
            if os.path.exists(checkpoint_path):
                model.load_state_dict(torch.load(checkpoint_path, map_location=device))
                print(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh t·ª´ {checkpoint_path}")
            else:
                print(f"‚ùå Kh√¥ng t√¨m th·∫•y checkpoint t·∫°i {checkpoint_path}")
                return
    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh hu·∫•n luy·ªán/t·∫£i m√¥ h√¨nh: {e}")
        return
    
    # T·∫°o 2 file CSV ri√™ng bi·ªát v√† so s√°nh ƒë·ªÉ c·∫£i thi·ªán PCC
    try:
        print("\nüìä T·∫°o file CSV d·ª± ƒëo√°n m√¥ h√¨nh v√† AHI PSG...")
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c results t·ªìn t·∫°i
        results_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'results')
        os.makedirs(results_dir, exist_ok=True)
        
        # 1. T·∫°o file CSV d·ª± ƒëo√°n m√¥ h√¨nh
        model_csv_path = os.path.join(results_dir, 'model_predictions.csv')
        model_df = create_model_predictions_csv(model, datasets, patient_ids, device, model_csv_path)
        
        # 2. T·∫°o file CSV AHI PSG (ground truth)
        psg_csv_path = os.path.join(results_dir, 'ahi_psg.csv')
        psg_df = create_ahi_psg_csv(datasets, patient_ids, psg_csv_path)
        
        # 3. So s√°nh 2 file CSV ƒë·ªÉ t√≠nh MAE, RMSE, PCC
        comparison_csv_path = os.path.join(results_dir, 'comparison_results.csv')
        comparison_df, metrics = compare_files_and_calculate_metrics(model_csv_path, psg_csv_path)
        
        if comparison_df is not None:
            print(f"\nüéØ SUMMARY METRICS (Dependent Subject):")
            print(f"  üìà MAE: {metrics['mae']:.2f}")
            print(f"  üìà RMSE: {metrics['rmse']:.2f}")  
            print(f"  üìà PCC: {metrics['pcc']:.4f}")
            print(f"  üìà R¬≤: {metrics['r2']:.4f}")
            print(f"  üìà Severity Accuracy: {metrics['severity_acc']:.2f}")
            
            print(f"\nüìÅ FILES ƒê∆Ø·ª¢C T·∫†O:")
            print(f"  üìÑ D·ª± ƒëo√°n m√¥ h√¨nh: {model_csv_path}")
            print(f"  üìÑ AHI PSG: {psg_csv_path}")
            print(f"  üìÑ So s√°nh chi ti·∫øt: {comparison_csv_path}")
        
        print("\n‚úÖ Ho√†n t·∫•t ƒë√°nh gi√° m√¥ h√¨nh Dependent Subject!")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o CSV v√† so s√°nh: {e}")
        traceback.print_exc()


def optimize_memory():
    """Gi·∫£i ph√≥ng b·ªô nh·ªõ cache v√† thu gom r√°c"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # Th√™m gi·∫£i ph√≥ng b·ªô nh·ªõ CUDA kh√¥ng s·ª≠ d·ª•ng
    torch.cuda.empty_cache()
    
    # ƒê·∫∑t bi·∫øn m√¥i tr∆∞·ªùng ƒë·ªÉ gi·ªõi h·∫°n cache PyTorch
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:32'


if __name__ == "__main__":
    try:
        # Ch·∫ø ƒë·ªô ƒë∆°n gi·∫£n gi·ªëng LSTM
        if len(sys.argv) == 1 or '--simple' in sys.argv:
            main_simple()
        else:
            # Ch·∫ø ƒë·ªô n√¢ng cao v·ªõi argparse
            print("\nüöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh ConvNeXtTransformerLite (Dependent Subject)...")
            parser = argparse.ArgumentParser(description='Hu·∫•n luy·ªán m√¥ h√¨nh ConvNeXtTransformerLite cho ph√°t hi·ªán ng∆∞ng th·ªü khi ng·ªß')
            parser.add_argument('--data_dir', type=str, help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c d·ªØ li·ªáu blocks')
            parser.add_argument('--batch_size', type=int, default=16, help='K√≠ch th∆∞·ªõc batch')
            parser.add_argument('--epochs', type=int, default=50, help='S·ªë epochs hu·∫•n luy·ªán')
            parser.add_argument('--learning_rate', type=float, default=2e-5, help='T·ªëc ƒë·ªô h·ªçc t·ªëi ∆∞u cho PCC cao')
            parser.add_argument('--weight_decay', type=float, default=0.005, help='Weight decay nh·∫π h∆°n cho PCC t·ªët')
            parser.add_argument('--device', type=str, default='cuda', help='Thi·∫øt b·ªã hu·∫•n luy·ªán (cuda/cpu)')
            parser.add_argument('--num_workers', type=int, default=0, help='S·ªë workers cho DataLoader')
            parser.add_argument('--pin_memory', action='store_true', help='S·ª≠ d·ª•ng pin_memory cho DataLoader')
            parser.add_argument('--eval_only', action='store_true', help='Ch·ªâ ƒë√°nh gi√° m√¥ h√¨nh, kh√¥ng hu·∫•n luy·ªán')
            parser.add_argument('--use_amp', action='store_true', help='S·ª≠ d·ª•ng Automatic Mixed Precision')
            parser.add_argument('--use_mixup', action='store_true', help='S·ª≠ d·ª•ng k·ªπ thu·∫≠t MixUp')
            parser.add_argument('--mixup_alpha', type=float, default=0.05, help='MixUp alpha nh·∫π h∆°n cho PCC t·ªët')
            parser.add_argument('--balance_classes', action='store_true', help='C√¢n b·∫±ng l·ªõp b·∫±ng WeightedRandomSampler')
            parser.add_argument('--use_swa', action='store_true', help='S·ª≠ d·ª•ng Stochastic Weight Averaging')
            parser.add_argument('--dropout', type=float, default=0.08, help='Dropout th·∫•p h∆°n cho PCC t·ªët')
            parser.add_argument('--use_early_stopping', action='store_true', help='S·ª≠ d·ª•ng early stopping')
            parser.add_argument('--patience', type=int, default=8, help='S·ªë epochs ch·ªù ƒë·ª£i c·∫£i thi·ªán tr∆∞·ªõc khi d·ª´ng')
            
            args = parser.parse_args()
            
            # ƒê·∫∑t seed cho t√°i t·∫°o k·∫øt qu·∫£
            set_seed(42)
            
            main()
        
        print("\n‚úÖ Ch∆∞∆°ng tr√¨nh ho√†n t·∫•t!")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        print("\nüîç Chi ti·∫øt l·ªói:")
        traceback.print_exc()
