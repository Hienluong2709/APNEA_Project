"""
Script hu·∫•n luy·ªán m√¥ h√¨nh ConvNeXt-LSTM v√† ConvNeXt-Transformer
phi√™n b·∫£n nh·∫π (~2-2.5M tham s·ªë) cho ph√°t hi·ªán Apnea
H·ªó tr·ª£ nhi·ªÅu ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu kh√°c nhau:
- Random split: Chia ng·∫´u nhi√™n kh√¥ng ph√¢n bi·ªát b·ªánh nh√¢n
- Dependent subject: Chia d·ªØ li·ªáu c·ªßa t·ª´ng b·ªánh nh√¢n th√†nh train/val
- Independent subject: Chia c√°c b·ªánh nh√¢n th√†nh nh√≥m train/val
"""
import os
import sys
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix

# Th√™m ƒë∆∞·ªùng d·∫´n g·ªëc v√†o sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# Import c√°c module
from models.convnext_lstm_lite import ConvNeXtLSTMLite
from models.convnext_transformer_lite import ConvNeXtTransformerLite
from dataset.lazy_apnea_dataset import LazyApneaDataset
from models.convnext_lite import count_parameters
from utils.data_splitting import (
    random_split_dataset,
    dependent_subject_split,
    independent_subject_split,
    get_dataloaders_from_split,
    save_split_info,
    get_data_distribution
)
try:
    from utils.visualization import plot_training_history, plot_confusion_matrix
except ImportError:
    print("‚ö†Ô∏è Module visualization kh√¥ng kh·∫£ d·ª•ng. S·∫Ω kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì.")
    plot_training_history = None
    plot_confusion_matrix = None

def train_model(model, train_loader, val_loader, epochs=10, lr=3e-4, device='cuda'):
    """
    Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh
    """
    print(f"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh {model.__class__.__name__} tr√™n {device}")
    print(f"üî¢ S·ªë l∆∞·ª£ng tham s·ªë: {count_parameters(model):,} ({count_parameters(model)/1e6:.2f}M)")
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)
    
    best_val_f1 = 0
    
    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán
    train_losses, val_losses = [], []
    train_accs, val_accs = [], []
    train_f1s, val_f1s = [], []
    
    for epoch in range(epochs):
        # TRAIN
        model.train()
        train_loss = 0
        all_preds, all_labels = [], []
        
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            preds = pred.argmax(1).detach().cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(y.cpu().numpy())
        
        train_loss /= len(train_loader)
        train_acc = accuracy_score(all_labels, all_preds)
        train_f1 = f1_score(all_labels, all_preds)
        
        # VALIDATE
        model.eval()
        val_loss = 0
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                pred = model(x)
                loss = criterion(pred, y)
                
                val_loss += loss.item()
                preds = pred.argmax(1).detach().cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(y.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = accuracy_score(all_labels, all_preds)
        val_f1 = f1_score(all_labels, all_preds)
        
        # C·∫≠p nh·∫≠t learning rate
        scheduler.step(val_f1)
        
        # L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            os.makedirs('checkpoints', exist_ok=True)
            torch.save(model.state_dict(), f'checkpoints/{model.__class__.__name__}_best.pth')
            
            # Chi ti·∫øt k·∫øt qu·∫£
            precision = precision_score(all_labels, all_preds)
            recall = recall_score(all_labels, all_preds)
            cm = confusion_matrix(all_labels, all_preds)
            print(f"Confusion Matrix:\n{cm}")
            print(f"Precision: {precision:.4f}, Recall: {recall:.4f}")
        
        # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)
        train_f1s.append(train_f1)
        val_f1s.append(val_f1)
        
        print(f"[Epoch {epoch+1}/{epochs}] "
              f"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | "
              f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}")
    
    # V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán
    if plot_training_history is not None:
        os.makedirs('results', exist_ok=True)
        # Bi·ªÉu ƒë·ªì Loss v√† F1
        plot_training_history(
            train_losses, val_losses, 
            train_f1s, val_f1s,
            metric_name='F1', 
            save_path=f'results/{model.__class__.__name__}_training_history.png'
        )
        
        # Bi·ªÉu ƒë·ªì Loss v√† Accuracy
        plot_training_history(
            train_losses, val_losses, 
            train_accs, val_accs,
            metric_name='Accuracy', 
            save_path=f'results/{model.__class__.__name__}_accuracy_history.png'
        )
    
    # L∆∞u l·ªãch s·ª≠ hu·∫•n luy·ªán d∆∞·ªõi d·∫°ng NumPy array
    os.makedirs('results', exist_ok=True)
    np.savez(
        f'results/{model.__class__.__name__}_history.npz',
        train_losses=np.array(train_losses),
        val_losses=np.array(val_losses),
        train_accs=np.array(train_accs),
        val_accs=np.array(val_accs),
        train_f1s=np.array(train_f1s),
        val_f1s=np.array(val_f1s)
    )
    
    return best_val_f1

def main(args):
    # ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu
    project_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    data_dir = os.path.join(project_dir, "data", "blocks")
    
    if not os.path.exists(data_dir):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c blocks t·∫°i {data_dir}")
        print("‚ö†Ô∏è Vui l√≤ng ch·∫°y build_dataset.py tr∆∞·ªõc")
        return
    
    # T·ªïng h·ª£p d·ªØ li·ªáu t·ª´ t·∫•t c·∫£ c√°c b·ªánh nh√¢n
    patient_dirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) 
                   if os.path.isdir(os.path.join(data_dir, d))]
    
    print(f"üìä ƒêang t·∫£i d·ªØ li·ªáu t·ª´ {len(patient_dirs)} b·ªánh nh√¢n")
    
    all_datasets = []
    patient_ids = []
    
    # T·∫£i d·ªØ li·ªáu c·ªßa t·ª´ng b·ªánh nh√¢n ri√™ng bi·ªát
    for p_dir in patient_dirs:
        patient_id = os.path.basename(p_dir)
        try:
            ds = LazyApneaDataset(p_dir)
            if len(ds) > 0:
                all_datasets.append(ds)
                patient_ids.append(patient_id)
                print(f"‚úÖ ƒê√£ t·∫£i {len(ds)} m·∫´u t·ª´ {patient_id}")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ {p_dir}: {e}")
    
    if not all_datasets:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán. Vui l√≤ng ch·∫°y build_dataset.py tr∆∞·ªõc.")
        return
    
    # Chia d·ªØ li·ªáu theo ph∆∞∆°ng ph√°p ƒë√£ ch·ªçn
    print(f"\nüìä Chia d·ªØ li·ªáu theo ph∆∞∆°ng ph√°p: {args.split_type}")
    
    if args.split_type == 'random':
        # Ph∆∞∆°ng ph√°p 1: Random split - chia ng·∫´u nhi√™n
        full_dataset = torch.utils.data.ConcatDataset(all_datasets)
        train_dataset, val_dataset = random_split_dataset(full_dataset, train_ratio=args.train_ratio, seed=args.seed)
        
        # L∆∞u th√¥ng tin chia d·ªØ li·ªáu
        save_split_info(
            output_dir='results',
            split_type='random',
            additional_info={
                'train_ratio': args.train_ratio,
                'seed': args.seed,
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset)
            }
        )
        
    elif args.split_type == 'dependent':
        # Ph∆∞∆°ng ph√°p 2: Dependent subject - chia d·ªØ li·ªáu c·ªßa t·ª´ng b·ªánh nh√¢n
        train_dataset, val_dataset = dependent_subject_split(
            datasets=all_datasets,
            patient_ids=patient_ids,
            train_ratio=args.train_ratio,
            seed=args.seed
        )
        
        # L∆∞u th√¥ng tin chia d·ªØ li·ªáu
        save_split_info(
            output_dir='results',
            split_type='dependent',
            train_patients=patient_ids,  # T·∫•t c·∫£ c√°c b·ªánh nh√¢n ƒë·ªÅu c√≥ d·ªØ li·ªáu trong c·∫£ train v√† val
            additional_info={
                'train_ratio': args.train_ratio,
                'seed': args.seed,
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset)
            }
        )
        
    elif args.split_type == 'independent':
        # Ph∆∞∆°ng ph√°p 3: Independent subject - chia b·ªánh nh√¢n th√†nh c√°c nh√≥m ri√™ng bi·ªát
        train_dataset, val_dataset, train_patients, val_patients = independent_subject_split(
            datasets=all_datasets,
            patient_ids=patient_ids,
            train_ratio=args.train_ratio,
            seed=args.seed
        )
        
        # L∆∞u th√¥ng tin chia d·ªØ li·ªáu
        save_split_info(
            output_dir='results',
            split_type='independent',
            train_patients=train_patients,
            val_patients=val_patients,
            additional_info={
                'train_ratio': args.train_ratio,
                'seed': args.seed,
                'train_samples': len(train_dataset),
                'val_samples': len(val_dataset)
            }
        )
    else:
        print(f"‚ùå Ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu {args.split_type} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        return
    
    # Ki·ªÉm tra ph√¢n b·ªë nh√£n
    print("\nüìä Ki·ªÉm tra ph√¢n b·ªë nh√£n:")
    train_dist = get_data_distribution(train_dataset)
    val_dist = get_data_distribution(val_dataset)
    
    print(f"Train: {train_dist}")
    print(f"Val: {val_dist}")
    
    # T√≠nh t·ªâ l·ªá ph√¢n b·ªë
    train_total = sum(train_dist.values())
    val_total = sum(val_dist.values())
    
    print("T·ªâ l·ªá ph√¢n b·ªë nh√£n:")
    for label in sorted(train_dist.keys()):
        train_pct = train_dist[label] / train_total * 100
        val_pct = val_dist[label] / val_total * 100 if label in val_dist else 0
        print(f"  Nh√£n {label}: Train {train_pct:.1f}%, Val {val_pct:.1f}%")
    
    # T·∫°o DataLoader
    train_loader, val_loader = get_dataloaders_from_split(
        train_dataset, 
        val_dataset, 
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        seed=args.seed
    )
    
    # Ch·ªçn thi·∫øt b·ªã
    device = torch.device(args.device if torch.cuda.is_available() and args.device == 'cuda' else 'cpu')
    print(f"\nüñ•Ô∏è S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    
    # Ch·ªçn v√† hu·∫•n luy·ªán m√¥ h√¨nh
    if args.model == 'lstm':
        model = ConvNeXtLSTMLite(num_classes=2).to(device)
    elif args.model == 'transformer':
        model = ConvNeXtTransformerLite(num_classes=2).to(device)
    else:
        print(f"‚ùå M√¥ h√¨nh {args.model} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£")
        return
    
    # T·∫°o th∆∞ m·ª•c k·∫øt qu·∫£ cho ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu hi·ªán t·∫°i
    result_dir = os.path.join('results', f'{args.split_type}_split')
    os.makedirs(result_dir, exist_ok=True)
    
    # Hu·∫•n luy·ªán
    best_val_f1 = train_model(
        model, 
        train_loader, 
        val_loader, 
        epochs=args.epochs, 
        lr=args.learning_rate,
        device=device
    )
    
    print(f"‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t. Best F1: {best_val_f1:.4f}")
    
    # ƒê√°nh gi√° tr√™n t·∫≠p validation v√† v·∫Ω confusion matrix
    if plot_confusion_matrix is not None:
        model.eval()
        all_preds, all_labels = [], []
        
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(device)
                outputs = model(x)
                preds = outputs.argmax(1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(y.numpy())
        
        # V·∫Ω confusion matrix
        os.makedirs(result_dir, exist_ok=True)
        plot_confusion_matrix(
            all_labels, 
            all_preds,
            classes=['Normal', 'Apnea'], 
            save_path=os.path.join(result_dir, f'{model.__class__.__name__}_confusion_matrix.png')
        )
        
        # L∆∞u k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng NumPy array
        np.savez(
            os.path.join(result_dir, f'{model.__class__.__name__}_predictions.npz'),
            y_true=np.array(all_labels),
            y_pred=np.array(all_preds)
        )

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hu·∫•n luy·ªán m√¥ h√¨nh ph√°t hi·ªán Apnea v·ªõi nhi·ªÅu ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu")
    parser.add_argument('--model', type=str, default='lstm', choices=['lstm', 'transformer'],
                        help='Lo·∫°i m√¥ h√¨nh ƒë·ªÉ hu·∫•n luy·ªán (lstm ho·∫∑c transformer)')
    parser.add_argument('--split_type', type=str, default='random', choices=['random', 'dependent', 'independent'],
                        help='Ph∆∞∆°ng ph√°p chia d·ªØ li·ªáu (random, dependent, independent)')
    parser.add_argument('--train_ratio', type=float, default=0.8, 
                        help='T·ª∑ l·ªá d·ªØ li·ªáu d√πng cho hu·∫•n luy·ªán (m·∫∑c ƒë·ªãnh: 0.8)')
    parser.add_argument('--batch_size', type=int, default=32, help='K√≠ch th∆∞·ªõc batch')
    parser.add_argument('--epochs', type=int, default=20, help='S·ªë epochs')
    parser.add_argument('--learning_rate', type=float, default=3e-4, help='Learning rate')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'],
                        help='Thi·∫øt b·ªã ƒë·ªÉ hu·∫•n luy·ªán (cuda ho·∫∑c cpu)')
    parser.add_argument('--num_workers', type=int, default=0, help='S·ªë worker cho DataLoader')
    parser.add_argument('--seed', type=int, default=42, help='Seed ng·∫´u nhi√™n')
    
    args = parser.parse_args()
    main(args)
